{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "from auxiliary import *\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import random\n",
    "import copy\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n",
      "0.104492\n",
      "100.0\n",
      "0.29516\n",
      "100.0\n",
      "0.030248\n",
      "100.0\n",
      "0.279928\n",
      "100.0\n",
      "0.011808\n",
      "100.0\n",
      "0.189708\n",
      "100.0\n",
      "0.005908\n",
      "100.0\n",
      "0.082748\n"
     ]
    }
   ],
   "source": [
    "easy_data_set = False # changer pour s√©lectionner un set de dim (1000,2) ou le vrai set\n",
    "data_subset = True # the data is divided in 8 subsets\n",
    "\n",
    "if data_subset:\n",
    "    r=8\n",
    "else:\n",
    "    r=1\n",
    "\n",
    "# declare all variables as lists\n",
    "ids=[0]*r\n",
    "y=[0]*r\n",
    "X=[0]*r\n",
    "x=[0]*r\n",
    "tx=[0]*r\n",
    "mean_x=[0]*r\n",
    "std_x=[0]*r\n",
    "N=[0]*r\n",
    "D=[0]*r\n",
    "prop=[0]*r\n",
    "\n",
    "if easy_data_set:\n",
    "    ids[0], y[0], X[0] = load_easy_data(sub_sample=False)\n",
    "else:\n",
    "    ids[0], y[0], X[0] = load_csv_data('../data/train.csv')\n",
    "    len_tot = X[0].shape[0]\n",
    "    if data_subset:\n",
    "        ids, y, X = split_data_boson(ids[0], X[0], y[0])\n",
    "\n",
    "    else:\n",
    "        # Only 100% clean data\n",
    "        ranges = [(1,4), (7,12), (13,23), (29,30)]\n",
    "        keep_idx = build_idx(ranges)\n",
    "        X[0] = X[0][:,keep_idx]\n",
    "\n",
    "\n",
    "for r in range(len(X)):\n",
    "    x[r], mean_x[r], std_x[r] = standardize(X[r])    # standardize\n",
    "    clean_data = [x for x in x[r] if not -999. in x] # check tidyness of data\n",
    "    print(len(clean_data)/len(x[r])*100) \n",
    "    tx[r] = build_poly(x[r], 1)                      # add column of 1's\n",
    "    N[r] = x[r].shape[0]\n",
    "    D[r] = x[r].shape[1]\n",
    "    prop[r] = len(x[r])/len_tot\n",
    "    print(prop[r])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "if data_subset:\n",
    "    r=8\n",
    "else:\n",
    "    r=1\n",
    "\n",
    "# declare variables\n",
    "ids_ukn=[0]*r\n",
    "y_ukn=[0]*r\n",
    "X_ukn=[0]*r\n",
    "x_ukn=[0]*r\n",
    "tx_ukn=[0]*r\n",
    "mean_x_ukn=[0]*r\n",
    "std_x_ukn=[0]*r\n",
    "\n",
    "\n",
    "# takes time, run only when needed\n",
    "ids_ukn[0], y_ukn[0], X_ukn[0] = load_csv_data('../data/test.csv')\n",
    "\n",
    "if data_subset:\n",
    "    ids_ukn, y_ukn, X_ukn = split_data_boson(ids_ukn[0], X_ukn[0], y_ukn[0])\n",
    "else:\n",
    "    ranges = [(1,4), (7,12), (13,23), (29,30)]\n",
    "    keep_idx = build_idx(ranges)\n",
    "    X_ukn[0] = X_ukn[0][:,keep_idx]\n",
    "\n",
    "\n",
    "for r in range(len(X_ukn)):\n",
    "    x_ukn[r], mean_x_ukn[r], std_x_ukn[r] = standardize(X_ukn[r])    # standardize\n",
    "    clean_data = [x for x in x_ukn[r] if not -999. in x]     # check tidyness of data\n",
    "    print(len(clean_data)/len(x_ukn[r])*100) \n",
    "    tx_ukn[r] = build_poly(x_ukn[r], 1)                      # add column of 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing data in 8 subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 100% Training\n",
    "On s'entraine sur 100% des datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if data_subset:\n",
    "    r=8\n",
    "else:\n",
    "    r=1\n",
    "\n",
    "loss=[0]*r\n",
    "w=[0]*r\n",
    "w_LS=[0]*r\n",
    "w_LS_GD=[0]*r\n",
    "w_LS_SGD=[0]*r\n",
    "w_LS_degree=[0]*r\n",
    "w_ridge_reg=[0]*r\n",
    "w_LR=[0]*r\n",
    "\n",
    "ratio_error_train=[0]*r\n",
    "ratio_error_test=[0]*r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.022707652148050421, 0.078046547757038051, 0.035009560260221242, 0.098022285159922506, 0.041224422936763094, 0.089890162893709352, 0.028470716296341923, 0.092875527585607548]\n"
     ]
    }
   ],
   "source": [
    "for r in range(len(x)):\n",
    "    tx[r] = build_poly(x[r], 1)\n",
    "    w_LS[r], loss[r] = least_squares(y[r],tx[r])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares (GD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_initial = w_LS\n",
    "\n",
    "max_iters = 100\n",
    "gammas = [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.2, 0.5]\n",
    "losses = np.zeros([len(gammas)])\n",
    "ws = []\n",
    "degree=1\n",
    "exp=0\n",
    "\n",
    "for r in range(len(x)):\n",
    "    tx[r] = build_poly(x[r], degree)\n",
    "\n",
    "for gamma in gammas:\n",
    "    for r in range(len(x)):\n",
    "        w_initial = np.array([0]*tx[r].shape[1])\n",
    "        w[r], loss[r] = least_squares_GD(y[r], tx[r], w_initial, max_iters, gamma)\n",
    "        losses[exp] += loss[r]*prop[r]\n",
    "    ws.append(copy.copy(w))\n",
    "    exp+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take experiment 10 out of 11 experiments, loss=0.07942102497394102, degree=1, gamma=0.2, max_iter=100\n"
     ]
    }
   ],
   "source": [
    "idx = np.argmin(losses) # get best experiment\n",
    "w_LS_GD = ws[idx]\n",
    "\n",
    "print(\"Take experiment {i} out of {tot} experiments, loss={l}, degree={deg}, gamma={ga}, max_iter={mi}\".format(\n",
    "           i=idx+1, tot=len(ws), l=losses[idx], deg=degree, ga=gammas[idx],mi=max_iters))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philippe/Desktop/ML_Projects/project1/phil/auxiliary.py:7: RuntimeWarning: overflow encountered in square\n",
      "  return 1/2 * np.mean(e**2)\n",
      "/Users/philippe/Desktop/ML_Projects/project1/phil/implementations.py:44: RuntimeWarning: invalid value encountered in subtract\n",
      "  w = w - gamma * gradients\n"
     ]
    }
   ],
   "source": [
    "max_iters = 1000\n",
    "gammas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.2, 0.4, 0.5]\n",
    "batch_size = 1\n",
    "losses = np.zeros([len(gammas)])\n",
    "ws = []\n",
    "exp=0\n",
    "degree=1\n",
    "\n",
    "for r in range(len(x)):\n",
    "    tx[r] = build_poly(x[r], degree)\n",
    "\n",
    "for gamma in gammas:\n",
    "    for r in range(len(x)):\n",
    "        w_initial = np.array([0]*(tx[r].shape[1]))\n",
    "        w[r], loss[r] = least_squares_SGD(y[r], tx[r], w_initial, batch_size, max_iters, gamma)\n",
    "        losses[exp] += loss[r]*prop[r]\n",
    "    ws.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ nan   0.   0.   0.   0.   0.   0.   0.]\n",
      "Take experiment 1 out of 8 experiments, loss=nan, gamma=1e-05\n"
     ]
    }
   ],
   "source": [
    "idx = np.argmin(losses) # get best experiment\n",
    "w_LS_SGD = ws[idx]\n",
    "\n",
    "print(\"Take experiment {i} out of {tot} experiments, loss={l}, gamma={g}\".format(\n",
    "           i=idx+1, tot=len(ws), l=losses[idx], g=gammas[idx]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Processing 0th experiment, degree=1, loss=0.093, err_ratio=0.235\n",
      "1: Processing 1th experiment, degree=3, loss=0.080, err_ratio=0.195\n",
      "2: Processing 2th experiment, degree=7, loss=0.073, err_ratio=0.177\n",
      "3: Processing 3th experiment, degree=10, loss=0.067, err_ratio=0.171\n",
      "4: Processing 4th experiment, degree=11, loss=0.067, err_ratio=0.169\n",
      "5: Processing 5th experiment, degree=12, loss=0.065, err_ratio=0.168\n",
      "6: Processing 6th experiment, degree=13, loss=0.067, err_ratio=0.188\n",
      "7: Processing 7th experiment, degree=14, loss=0.095, err_ratio=0.279\n",
      "8: Processing 8th experiment, degree=15, loss=20.948, err_ratio=0.236\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Constructing the polynomial basis function expansion of the data,\n",
    "   and then running least squares regression.\"\"\"\n",
    "# define parameters\n",
    "degrees = [1, 3, 7, 10, 11, 12, 13, 14, 15]\n",
    "losses = []\n",
    "ws = []\n",
    "ratio_errs = np.zeros([len(degrees)])\n",
    "deg = []\n",
    "exp=0\n",
    "        \n",
    "for ind, degree in enumerate(degrees):\n",
    "    for r in range(len(x)):\n",
    "        tx = build_poly(x[r], degree)  # form the data to do polynomial regression.\n",
    "        # least square and calculate RMSE\n",
    "        \n",
    "        w[r], loss_degree_LS = least_squares(y[r], tx)\n",
    "        error, ratio_error = compute_classification_error(y[r], tx, w[r], logistic_reg=False)\n",
    "        ratio_errs[exp] += ratio_error*prop[r]\n",
    "    \n",
    "    \n",
    "    losses.append(loss_degree_LS)\n",
    "    ws.append(copy.copy(w))\n",
    "    deg.append(degree)\n",
    "    print(\"{exp}: Processing {i}th experiment, degree={d}, loss={loss:.3f}, err_ratio={err:.3f}\".format(\n",
    "           exp=exp, i=ind, d=degree, loss=loss_degree_LS, err=ratio_errs[exp]))\n",
    "    exp+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take experiment 5 out of 9 experiments, degree=12, err_ratio=0.16785200000000003\n"
     ]
    }
   ],
   "source": [
    "idx = np.argmin(ratio_errs) # get best experiment\n",
    "w_degree_LS = ws[idx]\n",
    "\n",
    "\n",
    "print(\"Take experiment {i} out of {tot} experiments, degree={d}, err_ratio={err}\".format(\n",
    "           i=idx, tot=len(ws), d=deg[idx], err=ratio_errs[idx]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "To do with subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: degree=1, lambda=0.000, err_ratio=0.235\n",
      "1: degree=1, lambda=0.000, err_ratio=0.235\n",
      "2: degree=1, lambda=0.000, err_ratio=0.235\n",
      "3: degree=1, lambda=0.000, err_ratio=0.235\n",
      "4: degree=1, lambda=0.001, err_ratio=0.235\n",
      "5: degree=1, lambda=0.003, err_ratio=0.235\n",
      "6: degree=1, lambda=0.010, err_ratio=0.236\n",
      "7: degree=1, lambda=0.032, err_ratio=0.240\n",
      "8: degree=1, lambda=0.100, err_ratio=0.256\n",
      "9: degree=1, lambda=0.316, err_ratio=0.305\n",
      "10: degree=1, lambda=1.000, err_ratio=0.339\n",
      "11: degree=1, lambda=3.162, err_ratio=0.343\n",
      "12: degree=1, lambda=10.000, err_ratio=0.343\n",
      "13: degree=1, lambda=31.623, err_ratio=0.343\n",
      "14: degree=1, lambda=100.000, err_ratio=0.343\n",
      "15: degree=3, lambda=0.000, err_ratio=0.195\n",
      "16: degree=3, lambda=0.000, err_ratio=0.195\n",
      "17: degree=3, lambda=0.000, err_ratio=0.195\n",
      "18: degree=3, lambda=0.000, err_ratio=0.195\n",
      "19: degree=3, lambda=0.001, err_ratio=0.195\n",
      "20: degree=3, lambda=0.003, err_ratio=0.196\n",
      "21: degree=3, lambda=0.010, err_ratio=0.199\n",
      "22: degree=3, lambda=0.032, err_ratio=0.205\n",
      "23: degree=3, lambda=0.100, err_ratio=0.213\n",
      "24: degree=3, lambda=0.316, err_ratio=0.231\n",
      "25: degree=3, lambda=1.000, err_ratio=0.272\n",
      "26: degree=3, lambda=3.162, err_ratio=0.314\n",
      "27: degree=3, lambda=10.000, err_ratio=0.332\n",
      "28: degree=3, lambda=31.623, err_ratio=0.339\n",
      "29: degree=3, lambda=100.000, err_ratio=0.341\n",
      "30: degree=7, lambda=0.000, err_ratio=0.177\n",
      "31: degree=7, lambda=0.000, err_ratio=0.177\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-f0b9a6c56ff4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# form train and test data with polynomial basis function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# ridge regression with a given lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_classification_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogistic_reg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mratio_errs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mratio_error\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML_Projects/project1/phil/implementations.py\u001b[0m in \u001b[0;36mridge_regression\u001b[0;34m(y, tx, lambda_)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mlambda_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_prime\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mw_star\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "degrees = [1, 3, 7, 12]\n",
    "lambdas = np.logspace(-5, 2, 15)\n",
    "    \n",
    "ratio_errs = np.zeros([len(degrees)*len(lambdas)])\n",
    "ws = []\n",
    "deg=[]\n",
    "exp=0\n",
    "\n",
    "for degree in degrees:\n",
    "    for r in range(len(x)):\n",
    "        tx[r] = build_poly(x[r], degree)\n",
    "\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        for r in range(len(x)):\n",
    "            # form train and test data with polynomial basis function\n",
    "            # ridge regression with a given lambda\n",
    "            w[r], loss[r] = ridge_regression(y[r], tx[r], lambda_)\n",
    "            error, ratio_error = compute_classification_error(y[r], tx[r], w[r], logistic_reg=False)\n",
    "            ratio_errs[exp] += ratio_error*prop[r]\n",
    "\n",
    "        ws.append(copy.copy(w))\n",
    "    \n",
    "        deg.append(degree)\n",
    "\n",
    "        print(\"{exp}: degree={d}, lambda={l:.3f}, err_ratio={err_ratio:.3f}\".format(\n",
    "            exp=exp, d=degree, l=lambda_ , err_ratio=ratio_errs[exp]))\n",
    "        \n",
    "        exp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take experiment 48 out of 60 experiments, degree=12, err_ratio=0.167\n"
     ]
    }
   ],
   "source": [
    "idx = np.argmin(ratio_errs) # get best experiment\n",
    "w_ridge_reg = ws[idx]\n",
    "\n",
    "\n",
    "print(\"Take experiment {i} out of {tot} experiments, degree={d}, err_ratio={err:.3f}\".format(\n",
    "           i=idx, tot=len(ws), d=deg[idx], err=ratio_errs[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run experiment 1/8\n",
      "Run experiment 2/8\n",
      "Run experiment 3/8\n",
      "Run experiment 4/8\n",
      "Run experiment 5/8\n",
      "Run experiment 6/8\n",
      "Run experiment 7/8\n",
      "Run experiment 8/8\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "max_iters = 500\n",
    "gammas = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "#gammas = [1e-5, 1e-4]\n",
    "losses = np.zeros([len(gammas), len(x)])\n",
    "errors = np.zeros([len(gammas)])\n",
    "ws = []\n",
    "degree = 3\n",
    "\n",
    "for r in range(len(x)):\n",
    "    tx[r] = build_poly(x[r], degree)\n",
    "\n",
    "for idx_g, gamma in enumerate(gammas):\n",
    "    print(\"Run experiment {i}/{tot}\".format(i=idx_g+1, tot=len(gammas)))\n",
    "    for r in range(len(x)):\n",
    "        w_initial = np.array([0]*(tx[r].shape[1]))\n",
    "        w[r], loss[r] = logistic_regression(y[r], tx[r], w_initial, max_iters, gamma)\n",
    "        nb_errors, error_mean = compute_classification_error(y[r], tx[r], w[r], logistic_reg=True)\n",
    "        errors[idx_g] += error_mean*prop[r]\n",
    "    losses[idx_g, :] = loss[:]  \n",
    "    ws.append(copy.copy(w))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take experiments 3 out of 8 experiments, error=0.188, gamma=1e-06\n"
     ]
    }
   ],
   "source": [
    "idx = np.argmin(errors) # get best experiment\n",
    "\n",
    "w_LR = ws[idx]\n",
    "\n",
    "print(\"Take experiments {i} out of {tot} experiments, error={er:.3f}, gamma={g}\".format(\n",
    "           i=idx+1, tot=len(ws),  er=errors[idx], g=gammas[idx]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate for test set\n",
    "Sert √† afficher le nombre d'erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.186212\n"
     ]
    }
   ],
   "source": [
    "final_error_mean = 0.0\n",
    "\n",
    "tx=[0]*8\n",
    "\n",
    "for r in range(len(x)):\n",
    "    tx[r] = build_poly(x[r], 3)\n",
    "    w_selected = w_LR_CV[r]\n",
    "    nb_errors, error_mean = compute_classification_error(y[r], tx[r], w_selected, logistic_reg=True)\n",
    "    #print(nb_errors)\n",
    "    final_error_mean += error_mean*prop[r]\n",
    "print(final_error_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w_LS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d7da671b7e74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_LS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_LS_GD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_LS_SGD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_degree_LS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_ridge_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w_LS' is not defined"
     ]
    }
   ],
   "source": [
    "print(w_LS)\n",
    "print(w_LS_GD)\n",
    "print(w_LS_SGD)\n",
    "print(w_degree_LS)\n",
    "print(w_ridge_reg)\n",
    "print(w_LR)\n",
    "print(w_LR_CV)\n",
    "print(w_reg_log_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation\n",
    "Without k-fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation with least squares\n",
    "To do with subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 62500)\n",
      "exp=0, degree=1, Train loss, err_ratio=(0.088,0.266), Test loss, err_ratio=(0.088,0.266)\n",
      "exp=1, degree=3, Train loss, err_ratio=(0.081,0.239), Test loss, err_ratio=(1.627,0.240)\n",
      "exp=2, degree=7, Train loss, err_ratio=(0.079,0.225), Test loss, err_ratio=(82447866.810,0.225)\n",
      "exp=3, degree=11, Train loss, err_ratio=(0.075,0.206), Test loss, err_ratio=(355213321008866394112.000,0.207)\n",
      "exp=4, degree=12, Train loss, err_ratio=(0.075,0.205), Test loss, err_ratio=(4204608108250730594304.000,0.206)\n",
      "exp=5, degree=13, Train loss, err_ratio=(0.077,0.210), Test loss, err_ratio=(24543286522935610108280832.000,0.211)\n",
      "exp=6, degree=14, Train loss, err_ratio=(0.093,0.232), Test loss, err_ratio=(82609134220373679177004482560.000,0.232)\n"
     ]
    }
   ],
   "source": [
    "seed = 6\n",
    "degrees = [1, 3, 7, 11, 12, 13, 14]\n",
    "exp = 0\n",
    "\n",
    "loss_tr = []\n",
    "loss_te = []\n",
    "ratio_err_trains = []\n",
    "ratio_err_tests = []\n",
    "ws = []\n",
    "deg=[]\n",
    "\n",
    "k_fold = 4\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "print(k_indices.shape)\n",
    "\n",
    "for degree in degrees:\n",
    "    phi = build_poly(x, degree)\n",
    "    loss_tr_k_fold = []\n",
    "    loss_te_k_fold = []\n",
    "    ratio_err_trains_k_fold = []\n",
    "    ratio_err_tests_k_fold = []\n",
    "    ws_k_fold = []\n",
    "    for k in range(k_fold):\n",
    "        # split the data, and return train and test data: TODO\n",
    "        phi_train, phi_test, y_train, y_test = split_data_k_fold(phi, y, k_indices, k)\n",
    "            \n",
    "        # calcualte weight through least square.: TODO\n",
    "        w,loss = least_squares(y_train, phi_train)\n",
    "        ws_k_fold.append(w)\n",
    "\n",
    "        # calculate RMSE for train and test data,\n",
    "        # and store them in rmse_tr and rmse_te respectively: TODO\n",
    "        loss_tr_k_fold.append(compute_loss(y_train,phi_train,w), logistic_reg=False)\n",
    "        loss_te_k_fold.append(compute_loss(y_test,phi_test,w), logistic_reg=False)\n",
    "    \n",
    "        error, ratio_error_train = compute_classification_error(y_train, phi_train, w)\n",
    "        error, ratio_error_test = compute_classification_error(y_test, phi_test, w)\n",
    "        \n",
    "        ratio_err_trains_k_fold.append(ratio_error_train)\n",
    "        ratio_err_tests_k_fold.append(ratio_error_test)\n",
    "        \n",
    "    get_best = False # get best or get mean\n",
    "        \n",
    "    if get_best:\n",
    "        idx = np.argmin(ratio_err_tests_k_fold) # get best experiment\n",
    "        ws.append(ws_k_fold[idx])\n",
    "        loss_tr.append(loss_tr_k_fold[idx])\n",
    "        loss_te.append(loss_te_k_fold[idx])\n",
    "        ratio_err_trains.append(ratio_err_trains_k_fold[idx])\n",
    "        ratio_err_tests.append(ratio_err_tests_k_fold[idx])\n",
    "    else:\n",
    "        ws.append(sum(ws_k_fold)/len(ws_k_fold))\n",
    "        loss_tr.append(sum(loss_tr_k_fold)/len(loss_tr_k_fold))\n",
    "        loss_te.append(sum(loss_te_k_fold)/len(loss_te_k_fold))\n",
    "        ratio_err_trains.append(sum(ratio_err_trains_k_fold)/len(ratio_err_trains_k_fold))\n",
    "        ratio_err_tests.append(sum(ratio_err_tests_k_fold)/len(ratio_err_tests_k_fold))\n",
    "        \n",
    "    deg.append(degree)\n",
    "        \n",
    "    print(\"exp={e}, degree={d}, Train loss, err_ratio=({tr:.3f},{er_tr:.3f}), Test loss, err_ratio=({te:.3f},{er_te:.3f})\".format(\n",
    "        e=exp, d=degree, tr=loss_tr[exp], te=loss_te[exp], er_tr=ratio_err_trains[exp], er_te=ratio_err_tests[exp]))\n",
    "    exp += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation with ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-5, 2, 15)\n",
    "\n",
    "ratio = 0.9\n",
    "degrees = [1, 3, 7, 12]\n",
    "\n",
    "# split the data, and return train and test data\n",
    "    \n",
    "    \n",
    "loss_tr = []\n",
    "loss_te = []\n",
    "ratio_err_trains = []\n",
    "ratio_err_tests = []\n",
    "ws = []\n",
    "deg=[]\n",
    "\n",
    "for degree in degrees:\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        for r in range(len(x)):\n",
    "            # form train and test data with polynomial basis function\n",
    "            x_train, x_test, y_train, y_test = split_data(x[r], y[r], ratio)\n",
    "            phi_train = build_poly(x_train, degree)\n",
    "            phi_test = build_poly(x_test, degree)\n",
    "            # ridge regression with a given lambda\n",
    "            w[r], loss = ridge_regression(y_train, phi_train, lambda_)\n",
    "        ws.append(w)\n",
    "        loss_tr.append(compute_loss(y_train,phi_train,w))\n",
    "        loss_te.append(compute_loss(y_test,phi_test,w))\n",
    "    \n",
    "        error, ratio_error_train = compute_classification_error(y_train, phi_train, w, logistic_reg=False)\n",
    "        error, ratio_error_test = compute_classification_error(y_test, phi_test, w, logistic_reg=False)\n",
    "    \n",
    "        ratio_err_trains.append(ratio_error_train)\n",
    "        ratio_err_tests.append(ratio_error_test)\n",
    "        deg.append(degree)\n",
    "\n",
    "        print(\"proportion={p}, degree={d}, lambda={l:.3f}, Train loss,err_ratio=({tr:.3f},{err_ratio_tr:.3f}), Test loss,err_ratio=({te:.3f},{err_ratio_te:.3f})\".format(\n",
    "            p=ratio, d=degree, l=lambda_ ,tr=loss_tr[ind], te=loss_te[ind], err_ratio_tr=ratio_error_train, err_ratio_te=ratio_error_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_split(x, y, degree, ratio, gamma, seed):\n",
    "    \"\"\"polynomial regression with different split ratios and different degrees.\"\"\"\n",
    "    # split the data, and return train and test data: TODO\n",
    "    x_train, x_test, y_train, y_test = split_data(x, y, ratio, seed)\n",
    "\n",
    "    # form train and test data with polynomial basis function: TODO\n",
    "    phi_train = build_poly(x_train, degree)\n",
    "    phi_test = build_poly(x_test, degree)\n",
    "    \n",
    "    w_initial = np.array([0]*(phi_train.shape[1]))\n",
    "    max_iters = 1500\n",
    "\n",
    "    \n",
    "    # calcualte weight through least square.: TODO\n",
    "    w,loss_tr = logistic_regression(y_train, phi_train, w_initial, max_iters, gamma)\n",
    "    \n",
    "    loss_te = compute_logistic_loss(y_test, phi_test, w)\n",
    "    # compute error and loss for train and test data\n",
    "    error, ratio_error_train = compute_classification_error(y_train, phi_train, w)\n",
    "    error, ratio_error_test = compute_classification_error(y_test, phi_test, w)\n",
    "\n",
    "    \n",
    "    #print(\"proportion={p}, degree={d}, gamma={g}, Train loss, err_ratio=({tr:.3f},{er_tr:.3f}), Test loss, err_ratio=({te:.3f},{er_te:.3f})\".format(\n",
    "    #      p=ratio, d=degree, g=gamma, tr=loss_tr, te=loss_te, er_tr=ratio_error_train, er_te=ratio_error_test))\n",
    "    return w, ratio_error_train, ratio_error_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There will be 20 experiments\n",
      "0: proportion=0.2, degree=2, gamma=1e-06, Train err_ratio=0.188, Test err_ratio=0.193\n",
      "1: proportion=0.2, degree=2, gamma=1e-05, Train err_ratio=0.180, Test err_ratio=0.186\n",
      "2: proportion=0.2, degree=2, gamma=0.0001, Train err_ratio=0.231, Test err_ratio=0.236\n",
      "3: proportion=0.2, degree=2, gamma=0.001, Train err_ratio=0.231, Test err_ratio=0.236\n",
      "20: proportion=0.2, degree=3, gamma=1e-06, Train err_ratio=0.185, Test err_ratio=0.186\n",
      "21: proportion=0.2, degree=3, gamma=1e-05, Train err_ratio=0.179, Test err_ratio=0.182\n",
      "22: proportion=0.2, degree=3, gamma=0.0001, Train err_ratio=0.229, Test err_ratio=0.232\n",
      "23: proportion=0.2, degree=3, gamma=0.001, Train err_ratio=0.239, Test err_ratio=0.242\n",
      "40: proportion=0.2, degree=4, gamma=1e-06, Train err_ratio=0.203, Test err_ratio=0.205\n",
      "41: proportion=0.2, degree=4, gamma=1e-05, Train err_ratio=0.219, Test err_ratio=0.223\n",
      "42: proportion=0.2, degree=4, gamma=0.0001, Train err_ratio=0.234, Test err_ratio=0.236\n",
      "43: proportion=0.2, degree=4, gamma=0.001, Train err_ratio=0.227, Test err_ratio=0.233\n",
      "60: proportion=0.2, degree=5, gamma=1e-06, Train err_ratio=0.264, Test err_ratio=0.264\n",
      "61: proportion=0.2, degree=5, gamma=1e-05, Train err_ratio=0.265, Test err_ratio=0.271\n",
      "62: proportion=0.2, degree=5, gamma=0.0001, Train err_ratio=0.262, Test err_ratio=0.268\n",
      "break!\n",
      "63: proportion=0.2, degree=5, gamma=0.001, Train err_ratio=0.269, Test err_ratio=0.272\n",
      "80: proportion=0.2, degree=6, gamma=1e-06, Train err_ratio=0.319, Test err_ratio=0.322\n",
      "81: proportion=0.2, degree=6, gamma=1e-05, Train err_ratio=0.314, Test err_ratio=0.314\n",
      "break!\n",
      "82: proportion=0.2, degree=6, gamma=0.0001, Train err_ratio=0.323, Test err_ratio=0.324\n",
      "break!\n",
      "break!\n",
      "83: proportion=0.2, degree=6, gamma=0.001, Train err_ratio=0.298, Test err_ratio=0.301\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "seed = 6\n",
    "degrees = [2,3,4,5,6]\n",
    "split_ratios = [0.2]\n",
    "gammas = [1e-6, 1e-5, 1e-4, 1e-3]\n",
    "\n",
    "ws = []\n",
    "ratio_err_trains = []\n",
    "ratio_err_tests = []\n",
    "\n",
    "print(\"There will be {exp} experiments\".format(exp=len(degrees)*len(split_ratios)*len(gammas)))\n",
    "\n",
    "for idx0, split_ratio in enumerate(split_ratios):\n",
    "    for idx1, degree in enumerate(degrees):\n",
    "        for idx2, gamma in enumerate(gammas):\n",
    "            for r in range(len(x)): # compute the weight for each subset\n",
    "                w[r], ratio_error_train[r], ratio_error_test[r] = logistic_regression_split(x[r], y[r], degree, split_ratio, gamma, seed)\n",
    "            ws.append(copy.copy(w))\n",
    "            ratio_err_trains.append(0)\n",
    "            ratio_err_tests.append(0)\n",
    "            for r in range(len(ratio_error_train)):\n",
    "                ratio_err_trains[-1]+= ratio_error_train[r]*prop[r]\n",
    "                ratio_err_tests[-1] += ratio_error_test[r]*prop[r]\n",
    "            print(\"{exp}: proportion={p}, degree={d}, gamma={g}, Train err_ratio={er_tr:.3f}, Test err_ratio={er_te:.3f}\".format(\n",
    "                   exp=idx0*len(split_ratios)*len(degrees)*len(gammas)+idx1*len(degrees)*len(gammas)+idx2, p=split_ratio, d=degree, g=gamma, \n",
    "                   er_tr=ratio_err_trains[-1], er_te=ratio_err_tests[-1]))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take experiments 5 out of 20 experiments, error tr=0.179, error test=0.182, split_ratio=0.2, degree=3, gamma=1e-05\n"
     ]
    }
   ],
   "source": [
    "idx = np.argmin(ratio_err_tests) # get best experiment\n",
    "w_LR_CV = ws[idx]\n",
    "\n",
    "\n",
    "idx_split = int(idx/(len(degrees)*len(gammas)))\n",
    "idx_degree_gamma = idx%(len(degrees)*len(gammas))\n",
    "idx_degree = int(idx_degree_gamma/len(gammas))\n",
    "idx_gamma = (idx_degree_gamma%len(gammas))\n",
    "\n",
    "\n",
    "print(\"Take experiments {i} out of {tot} experiments, error tr={er_tr:.3f}, error test={er_te:.3f}, split_ratio={spl}, degree={deg}, gamma={g}\".format(\n",
    "           i=idx, tot=len(ws), er_tr=ratio_err_trains[idx], er_te=ratio_err_tests[idx], \n",
    "           spl=split_ratios[idx_split],  deg=degrees[idx_degree], g=gammas[idx_gamma]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation with regularized logistic regression\n",
    "To do with subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reg_logistic_regression_split(x, y, degree, ratio, gamma, lambda_, seed):\n",
    "    \"\"\"polynomial regression with different split ratios and different degrees.\"\"\"\n",
    "    # split the data, and return train and test data: TODO\n",
    "    x_train, x_test, y_train, y_test = split_data(x, y, ratio, seed)\n",
    "\n",
    "    # form train and test data with polynomial basis function: TODO\n",
    "    phi_train = build_poly(x_train, degree)\n",
    "    phi_test = build_poly(x_test, degree)\n",
    "    \n",
    "    w_initial = np.array([0]*(phi_train.shape[1]))\n",
    "    max_iters = 1000\n",
    "\n",
    "    \n",
    "    # calcualte weight through logistic regression\n",
    "    w, loss_tr = reg_logistic_regression(y_train, phi_train, lambda_, w_initial, max_iters, gamma, SGD=False)\n",
    "    \n",
    "    loss_te = compute_logistic_loss(y_test, phi_test, w)\n",
    "    # compute error and loss for train and test data\n",
    "    error, ratio_error_train = compute_classification_error(y_train, phi_train, w)\n",
    "    error, ratio_error_test = compute_classification_error(y_test, phi_test, w)\n",
    "\n",
    "    \n",
    "    #print(\"prop={p}, deg={d}, g={g:.3f}, l={l:.3f}, Train loss, err_ratio=({tr:.3f},{er_tr:.3f}), Test loss, err_ratio=({te:.3f},{er_te:.3f})\".format(\n",
    "    #      p=ratio, d=degree, g=gamma, l=lambda_, tr=loss_tr, te=loss_te, er_tr=ratio_error_train, er_te=ratio_error_test))\n",
    "    return w, ratio_error_train, ratio_error_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There will be 12 experiments\n",
      "1: proportion=0.2, degree=2, gamma=5e-07, lambda=0.01, Train err_ratio=0.207, Test err_ratio=0.209\n",
      "2: proportion=0.2, degree=2, gamma=1e-06, lambda=0.01, Train err_ratio=0.202, Test err_ratio=0.199\n",
      "3: proportion=0.2, degree=2, gamma=5e-06, lambda=0.01, Train err_ratio=0.189, Test err_ratio=0.190\n",
      "4: proportion=0.2, degree=3, gamma=5e-07, lambda=0.01, Train err_ratio=0.204, Test err_ratio=0.206\n",
      "5: proportion=0.2, degree=3, gamma=1e-06, lambda=0.01, Train err_ratio=0.197, Test err_ratio=0.198\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-ded5df53f771>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                     \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio_error_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio_error_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_logistic_regression_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mratio_err_trains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-114-29ff3f6a295c>\u001b[0m in \u001b[0;36mreg_logistic_regression_split\u001b[0;34m(x, y, degree, ratio, gamma, lambda_, seed)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# calcualte weight through logistic regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mloss_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_logistic_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML_Projects/project1/phil/implementations.py\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[0;34m(y, tx, lambda_, initial_w, max_iters, gamma, SGD, batch_size)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_logistic_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_logistic_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mloss_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML_Projects/project1/phil/auxiliary.py\u001b[0m in \u001b[0;36mcompute_logistic_loss\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_logistic_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# gives NaN ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m#loss = np.sum(np.log(1.0+np.exp(tx.dot(w))) - y*(tx.dot(w)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed = 6\n",
    "degrees = [2, 3, 4, 5]\n",
    "split_ratios = [0.2]\n",
    "gammas = [0.5e-6, 1e-6, 0.5e-5]\n",
    "lambdas = [0.01]#np.logspace(-3, 1, 2)\n",
    "\n",
    "ws = []\n",
    "ratio_err_trains = []\n",
    "ratio_err_tests = []\n",
    "\n",
    "print(\"There will be {exp} experiments\".format(exp=len(degrees)*len(split_ratios)*len(gammas)*len(lambdas)))\n",
    "\n",
    "\n",
    "for idx0, split_ratio in enumerate(split_ratios):\n",
    "    for idx1, degree in enumerate(degrees):\n",
    "        for idx2, gamma in enumerate(gammas):\n",
    "            for idx3, lambda_ in enumerate(lambdas):\n",
    "                for r in range(len(x)):\n",
    "                    w[r], ratio_error_train[r], ratio_error_test[r] = reg_logistic_regression_split(x[r], y[r], degree, split_ratio, gamma, lambda_, seed)\n",
    "                ws.append(copy.copy(w))\n",
    "                ratio_err_trains.append(0)\n",
    "                ratio_err_tests.append(0)\n",
    "                for r in range(len(ratio_error_train)):\n",
    "                    ratio_err_trains[-1]+= ratio_error_train[r]*prop[r]\n",
    "                    ratio_err_tests[-1] += ratio_error_test[r]*prop[r]\n",
    "                print(\"{exp}: proportion={p}, degree={d}, gamma={g}, lambda={l}, Train err_ratio={er_tr:.3f}, Test err_ratio={er_te:.3f}\".format(\n",
    "                       exp=idx0*len(degrees)*len(gammas)*len(lambdas)\n",
    "                          +idx1*len(gammas)*len(lambdas)\n",
    "                          +idx2*len(lambdas)+idx3+1, \n",
    "                       p=split_ratio, d=degree, g=gamma, l=lambda_,\n",
    "                       er_tr=ratio_err_trains[-1], er_te=ratio_err_tests[-1]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take experiments 2 out of 5 experiments, error tr=0.189, error test=0.190, split_ratio=0.2, degree=2, gamma=5e-07\n"
     ]
    }
   ],
   "source": [
    "idx = np.argmin(ratio_err_tests)\n",
    "w_reg_log_reg = ws[idx]\n",
    "\n",
    "\n",
    "idx_split = int(idx/(len(degrees)*len(gammas)*len(lambdas)))\n",
    "idx_degree_gamma_lamb = idx%(len(degrees)*len(gammas)*len(lambdas))\n",
    "idx_degree = int(idx_degree_gamma_lamb/(len(gammas)*len(lambdas)))\n",
    "idx_gamma_lamb = idx_degree_gamma_lamb%(len(gammas)*len(lambdas))\n",
    "idx_gamma = int(idx_gamma_lamb/len(gammas))\n",
    "idx_lamb = (idx_gamma_lamb%len(lambdas))\n",
    "\n",
    "\n",
    "print(\"Take experiments {i} out of {tot} experiments, error tr={er_tr:.3f}, error test={er_te:.3f}, split_ratio={spl}, degree={deg}, gamma={g}\".format(\n",
    "           i=idx, tot=len(ws), er_tr=ratio_err_trains[idx], er_te=ratio_err_tests[idx], \n",
    "           spl=split_ratios[idx_split],  deg=degrees[idx_degree], g=gammas[idx_gamma]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "z = [1, 0, 2, 3]\n",
    "print(np.argmin(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation (with k-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if data_subset:\n",
    "    r=8\n",
    "else:\n",
    "    r=1\n",
    "\n",
    "loss=[0]*r\n",
    "w=[0]*r\n",
    "w_LS=[0]*r\n",
    "w_LS_GD=[0]*r\n",
    "w_LS_SGD=[0]*r\n",
    "w_LS_degree=[0]*r\n",
    "w_ridge_reg=[0]*r\n",
    "w_LR=[0]*r\n",
    "phi=[0]*r\n",
    "\n",
    "ratio_error_train=[0]*r\n",
    "ratio_error_test=[0]*r\n",
    "\n",
    "k_indices=[0]*r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: degree=1, Train err_ratio=0.235, Test err_ratio=0.234\n",
      "1: degree=3, Train err_ratio=0.194, Test err_ratio=0.194\n",
      "2: degree=7, Train err_ratio=0.175, Test err_ratio=0.178\n",
      "3: degree=11, Train err_ratio=0.168, Test err_ratio=0.170\n",
      "4: degree=12, Train err_ratio=0.168, Test err_ratio=0.172\n",
      "5: degree=13, Train err_ratio=0.167, Test err_ratio=0.172\n",
      "6: degree=14, Train err_ratio=0.237, Test err_ratio=0.239\n"
     ]
    }
   ],
   "source": [
    "seed = 6\n",
    "degrees = [1, 3, 7, 11, 12, 13, 14]\n",
    "exp = 0\n",
    "\n",
    "loss_tr = []\n",
    "loss_te = []\n",
    "ratio_err_trains = []\n",
    "ratio_err_tests  = []\n",
    "ws = []\n",
    "deg=[]\n",
    "\n",
    "k_fold = 4\n",
    "\n",
    "for r in range(len(x)):\n",
    "    k_indices[r] = build_k_indices(y[r], k_fold, seed)\n",
    "\n",
    "for degree in degrees:\n",
    "    for r in range(len(x)):\n",
    "        phi[r] = build_poly(x[r], degree)\n",
    "        \n",
    "\n",
    "    ratio_err_trains_k_fold = [0]*k_fold\n",
    "    ratio_err_tests_k_fold  = [0]*k_fold\n",
    "    ws_k_fold = []\n",
    "    for k in range(k_fold):\n",
    "        for r in range(len(x)):\n",
    "            # split the data, and return train and test data: TODO\n",
    "            phi_train, phi_test, y_train, y_test = split_data_k_fold(phi[r], y[r], k_indices[r], k)\n",
    "            \n",
    "            # calcualte weight through least square.: TODO\n",
    "            w[r],_ = least_squares(y_train, phi_train)\n",
    "            \n",
    "            _, ratio_error_train = compute_classification_error(y_train, phi_train, w[r], logistic_reg=False)\n",
    "            _, ratio_error_test = compute_classification_error(y_test, phi_test, w[r], logistic_reg=False)\n",
    "            \n",
    "            ratio_err_trains_k_fold[k] += ratio_error_train*prop[r]\n",
    "            ratio_err_tests_k_fold[k]  += ratio_error_test*prop[r]\n",
    "\n",
    "        ws_k_fold.append(copy.copy(w))\n",
    "\n",
    " \n",
    "    idx = np.argmin(ratio_err_tests_k_fold) # get best experiment\n",
    "    ws.append(ws_k_fold[idx])\n",
    "    ratio_err_trains.append(ratio_err_trains_k_fold[idx])\n",
    "    ratio_err_tests.append(ratio_err_tests_k_fold[idx])\n",
    "    \n",
    "        \n",
    "    deg.append(degree)\n",
    "        \n",
    "    print(\"{e}: degree={d}, Train err_ratio={er_tr:.3f}, Test err_ratio={er_te:.3f}\".format(\n",
    "        e=exp, d=degree,er_tr=ratio_err_trains[exp], er_te=ratio_err_tests[exp]))\n",
    "    exp += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take experiment 3 out of 7 experiments, degree=11, err_ratio=0.17028090316174446\n"
     ]
    }
   ],
   "source": [
    "idx = np.argmin(ratio_err_tests) # get best experiment\n",
    "w_LS_degree_CV = ws[idx]\n",
    "\n",
    "\n",
    "print(\"Take experiment {i} out of {tot} experiments, degree={d}, err_ratio={err}\".format(\n",
    "           i=idx, tot=len(ws), d=deg[idx], err=ratio_err_tests[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reg_logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "There will be 240 experiments\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-12c1c16fbb2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m                     \u001b[0;31m#tdiprint(r, np.asarray(y[r]).shape, np.asarray(y[r]).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                     w[r], ratio_error_train[r], ratio_error_test[r] = reg_logistic_regression_split_k_fold(\n\u001b[0;32m---> 55\u001b[0;31m                         x[r], y[r], degree, gamma, lambda_, k_indices, k)\n\u001b[0m\u001b[1;32m     56\u001b[0m                 \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mratio_err_trains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-12c1c16fbb2e>\u001b[0m in \u001b[0;36mreg_logistic_regression_split_k_fold\u001b[0;34m(x, y, degree, gamma, lambda_, k_indices, k)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# calcualte weight through least square.: TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mloss_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_logistic_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML_Projects/project1/phil/implementations.py\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[0;34m(y, tx, lambda_, initial_w, max_iters, gamma, SGD, batch_size)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_logistic_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_logistic_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mloss_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML_Projects/project1/phil/auxiliary.py\u001b[0m in \u001b[0;36mcompute_logistic_loss\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_logistic_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# gives NaN ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;31m#loss = np.sum(np.log(1.0+np.exp(tx.dot(w))) - y*(tx.dot(w)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;31m#[0][0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def reg_logistic_regression_split_k_fold(x, y, degree, gamma, lambda_, k_indices, k):\n",
    "    \"\"\"polynomial regression with different split ratios and different degrees.\"\"\"\n",
    "    # split the data, and return train and test data: TODO\n",
    "    x_train, x_test, y_train, y_test = split_data_k_fold(x, y, k_indices, k)\n",
    "\n",
    "    # form train and test data with polynomial basis function: TODO\n",
    "    phi_train = build_poly(x_train, degree)\n",
    "    phi_test = build_poly(x_test, degree)\n",
    "\n",
    "    w_initial = np.array([0]*(phi_train.shape[1]))\n",
    "    max_iters = 1000\n",
    "\n",
    "\n",
    "    # calcualte weight through least square.: TODO\n",
    "    w,loss_tr = reg_logistic_regression(y_train, phi_train, lambda_, w_initial, max_iters, gamma)\n",
    "\n",
    "    loss_te = compute_logistic_loss(y_test, phi_test, w)\n",
    "    # compute error and loss for train and test data\n",
    "    error, ratio_error_train = compute_classification_error(y_train, phi_train, w)\n",
    "    error, ratio_error_test = compute_classification_error(y_test, phi_test, w)\n",
    "\n",
    "\n",
    "    #print(\"proportion={p}, degree={d}, gamma={g}, Train loss, err_ratio=({tr:.3f},{er_tr:.3f}), Test loss, err_ratio=({te:.3f},{er_te:.3f})\".format(\n",
    "    #      p=ratio, d=degree, g=gamma, tr=loss_tr, te=loss_te, er_tr=ratio_error_train, er_te=ratio_error_test))\n",
    "    return w, ratio_error_train, ratio_error_test\n",
    "\n",
    "\n",
    "seed = 6\n",
    "degrees = [1, 2, 3, 4]\n",
    "gammas = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "lambdas = [0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "\n",
    "ws = []\n",
    "ratio_err_trains = []\n",
    "ratio_err_tests = []\n",
    "\n",
    "k_indices=[0]*8\n",
    "for r in range(len(x)):\n",
    "    k_indices[r] = build_k_indices(y[r], k_fold, seed)\n",
    "\n",
    "k_fold = 3\n",
    "print(len(k_indices[0]))\n",
    "\n",
    "print(\"There will be {exp} experiments\".format(exp=len(degrees)*k_fold*len(gammas)*len(lambdas)))\n",
    "\n",
    "for idx1, degree in enumerate(degrees):\n",
    "    for idx2, gamma in enumerate(gammas):\n",
    "        for idx3, lambda_ in enumerate(lambdas):\n",
    "            for k in range(k_fold):\n",
    "                for r in range(len(x)):\n",
    "                    k_indices = build_k_indices(y[r], k_fold, seed)\n",
    "                    #tdiprint(r, np.asarray(y[r]).shape, np.asarray(y[r]).shape)\n",
    "                    w[r], ratio_error_train[r], ratio_error_test[r] = reg_logistic_regression_split_k_fold(\n",
    "                        x[r], y[r], degree, gamma, lambda_, k_indices, k)\n",
    "                ws.append(copy.copy(w))\n",
    "                ratio_err_trains.append(0)\n",
    "                ratio_err_tests.append(0)\n",
    "                for r in range(len(ratio_error_train)):\n",
    "                    ratio_err_trains[-1]+= ratio_error_train[r]*prop[r]\n",
    "                    ratio_err_tests[-1] += ratio_error_test[r]*prop[r]\n",
    "                print(\"{exp}: degree={d}, gamma={g}, lambda={l}, k={k}/{k_f}, Train err_ratio={er_tr:.3f}, Test err_ratio={er_te:.3f}\".format(\n",
    "                       exp=idx1*len(gammas)*len(lambdas)*k+idx2*len(lambdas)*k+idx3*k+k, \n",
    "                       d=degree, g=gamma, k=k+1, k_f=k_fold, l=lambda_,\n",
    "                       er_tr=ratio_err_trains[-1], er_te=ratio_err_tests[-1]))\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform estimation on evaluation set (for kaggle)\n",
    "Une fois qu'on a le bon vecteur de poids, il suffit de runner ce code pour avoir le fichier de sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_write = np.array([])\n",
    "y_to_write = np.array([])\n",
    "\n",
    "for r in range(len(x_ukn)):\n",
    "    tx_ukn[r] = build_poly(x_ukn[r], 3) # put the right degree\n",
    "    w_selected = w_LR_CV[r]             # put the desired weight vector\n",
    "    y_ukn[r] = predict_labels(w_selected, tx_ukn[r], logistic_reg=True) # pay attention to the value of the last argument\n",
    "    y_ukn[r][y_ukn[r]==0] = -1\n",
    "    \n",
    "    ids_to_write = np.concatenate([ids_to_write, ids_ukn[r]], axis=0)\n",
    "    y_to_write = np.concatenate([y_to_write, y_ukn[r]], axis=0)\n",
    "    \n",
    "create_csv_submission(ids_to_write, y_to_write, 'submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
