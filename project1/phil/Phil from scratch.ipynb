{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "from auxiliary import *\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "easy_data_set = False # changer pour sélectionner un set de dim (1000,2) ou le vrai set\n",
    "\n",
    "if easy_data_set:\n",
    "    ids, y, X = load_easy_data(sub_sample=False)\n",
    "else:\n",
    "    ids, y, X = load_csv_data('../data/train.csv')\n",
    "    # Only 100% clean data\n",
    "    ranges = [(1,4), (7,12), (13,23), (29,30)]\n",
    "    keep_idx = build_idx(ranges)\n",
    "    X = X[:,keep_idx]\n",
    "\n",
    "x, mean_x, std_x = standardize(X)\n",
    "tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "tx = build_poly(x, 1)\n",
    "\n",
    "N = x.shape[0]\n",
    "D = x.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to chekc that we eliminated all unecessary features\n",
    "clean_data = [x for x in X if not -999. in x]\n",
    "len(clean_data)/len(X)*100 # percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0., ...,  1.,  0.,  0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# takes time, run only when needed\n",
    "ids_ukn, y_ukn, X_ukn = load_csv_data('../data/test.csv')\n",
    "\n",
    "ranges = [(1,4), (7,12), (13,23), (29,30)]\n",
    "keep_idx = build_idx(ranges)\n",
    "X_ukn = X_ukn[:,keep_idx]\n",
    "\n",
    "x_ukn, mean_x_ukn, std_x_ukn = standardize(X_ukn)\n",
    "tx_ukn = build_poly(x_ukn, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training\n",
    "Différents types de training possibles. Normalement, seul le logistic regression a du sens. Les autres sont là pour pouvoir tester les diverses fonctions à fournir dans `implementation.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses_per_algorithm = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0881281775747\n"
     ]
    }
   ],
   "source": [
    "w_LS, loss = least_squares(y,tx)\n",
    "print(loss)\n",
    "losses_per_algorithm['LS'] = loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares (GD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# w_initial = w_LS\n",
    "w_initial = np.array([0]*(D+1))\n",
    "max_iters = 100\n",
    "gammas = [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.2, 0.5]\n",
    "losses = []\n",
    "ws = [] \n",
    "\n",
    "for gamma in gammas:\n",
    "    w, loss = least_squares_GD(y, tx, w_initial, max_iters, gamma)\n",
    "    losses.append(loss)\n",
    "    ws.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(losses)\n",
    "w_LS_GD = ws[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-42766f4e8bfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgammas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0msgd_ws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares_SGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msgd_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msgd_ws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Dylan/Documents/Cours/MT_MA3/Machine Learning/ML_Projects/project1/phil/implementations.py\u001b[0m in \u001b[0;36mleast_squares_SGD\u001b[0;34m(y, tx, initial_w, batch_size, max_iters, gamma)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mminibatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_tx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# avant: minibatch_y et minibatch_tx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_tx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Dylan/Documents/Cours/MT_MA3/Machine Learning/ML_Projects/project1/phil/auxiliary.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompute_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w_initial = np.array([0]*(D+1))\n",
    "max_iters = 1000\n",
    "gammas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.2, 0.4, 0.5]\n",
    "batch_size = 10\n",
    "losses = []\n",
    "ws = []\n",
    "\n",
    "for gamma in gammas:\n",
    "    sgd_ws, sgd_losses = least_squares_SGD(y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "    losses.append(sgd_losses)\n",
    "    ws.append(sgd_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(losses)\n",
    "print(len(ws))\n",
    "w_LS_SGD = ws[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Constructing the polynomial basis function expansion of the data,\n",
    "   and then running least squares regression.\"\"\"\n",
    "# define parameters\n",
    "degrees = [1, 3, 7, 12]\n",
    "losses = []\n",
    "ws = []\n",
    "    \n",
    "phi = build_poly(x, 2)  # form the data to do polynomial regression.\n",
    "print(x[0,:])\n",
    "print(phi[0,:])\n",
    "    \n",
    "for ind, degree in enumerate(degrees):\n",
    "    phi = build_poly(x, degree)  # form the data to do polynomial regression.\n",
    "    # least square and calculate RMSE\n",
    "        \n",
    "    w_degree_LS, loss_degree_LS = least_squares(y, phi)\n",
    "        \n",
    "    losses.append(loss_degree_LS)\n",
    "    ws.append(w_degree_LS)\n",
    "    print(\"Processing {i}th experiment, degree={d}, loss={loss}\".format(\n",
    "           i=ind + 1, d=degree, loss=loss_degree_LS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_degree_LS = ws[np.argmin(losses)]\n",
    "print(np.argmin(losses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "A tester!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-5, 0, 15)\n",
    "\n",
    "ratio = 0.5 \n",
    "degree = 7\n",
    "\n",
    "# split the data, and return train and test data\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "    \n",
    "# form train and test data with polynomial basis function\n",
    "phi_train = build_poly(x_train, degree)\n",
    "phi_test = build_poly(x_test, degree)\n",
    "    \n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    # ridge regression with a given lambda\n",
    "    w, loss = ridge_regression(y_train, phi_train, lambda_)\n",
    "    rmse_tr.append(compute_loss(y_train,phi_train,w))\n",
    "    rmse_te.append(compute_loss(y_test,phi_test,w))\n",
    "\n",
    "    print(\"proportion={p}, degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "            p=ratio, d=degree, l=lambda_ ,tr=rmse_tr[ind], te=rmse_te[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_initial = np.array([0]*(D+1))\n",
    "max_iters = 1000\n",
    "gammas = [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.2, 0.5]\n",
    "gammas = [0.01]\n",
    "losses = []\n",
    "ws = []\n",
    "\n",
    "for gamma in gammas:\n",
    "    w, loss = logistic_regression(y, tx, w_initial, max_iters, gamma)\n",
    "    losses.append(loss)\n",
    "    ws.append(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(losses)\n",
    "w_LR = ws[0]\n",
    "print(w_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate for test set\n",
    "Sert à afficher le nombre d'erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tx = build_poly(x, 1)\n",
    "nb_errors, error_mean = compute_classification_error(y, tx, w_reg_log_reg)\n",
    "print(nb_errors)\n",
    "print(error_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(w_LS)\n",
    "print(w_LS_GD)\n",
    "print(w_LS_SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation with least squares\n",
    "Je l'ai fait pour la beauté du geste, mais normalement il ne faudra pas utiliser ce bloc de code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_split_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"polynomial regression with different split ratios and different degrees.\"\"\"\n",
    "    # split the data, and return train and test data: TODO\n",
    "    x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "\n",
    "    # form train and test data with polynomial basis function: TODO\n",
    "    phi_train = build_poly(x_train, degree)\n",
    "    phi_test = build_poly(x_test, degree)\n",
    "    \n",
    "    # calcualte weight through least square.: TODO\n",
    "    w,loss = least_squares(y_train, phi_train)\n",
    "\n",
    "    # calculate RMSE for train and test data,\n",
    "    # and store them in rmse_tr and rmse_te respectively: TODO\n",
    "    loss_tr = compute_loss(y_train,phi_train,w)\n",
    "    loss_te = compute_loss(y_test,phi_test,w)\n",
    "    \n",
    "    print(\"proportion={p}, degree={d}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "          p=ratio, d=degree, tr=loss_tr, te=loss_te))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 6\n",
    "degrees = [1, 3, 7, 12]\n",
    "split_ratios = [0.9, 0.5, 0.1]\n",
    "\n",
    "for split_ratio in split_ratios:\n",
    "    for degree in degrees:\n",
    "        train_test_split_demo(x, y, degree, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation with logistic regression\n",
    "Les choses sérieuses commencent ici avec des tests sur le logistic regression. Cependant, on peut directement passer à la section suivante, qui est une généralisation (il y a le \"lambda\" en plus) de ce code-ci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_split(x, y, degree, ratio, gamma, seed):\n",
    "    \"\"\"polynomial regression with different split ratios and different degrees.\"\"\"\n",
    "    # split the data, and return train and test data: TODO\n",
    "    x_train, x_test, y_train, y_test = split_data(x, y, ratio, seed)\n",
    "\n",
    "    # form train and test data with polynomial basis function: TODO\n",
    "    phi_train = build_poly(x_train, degree)\n",
    "    phi_test = build_poly(x_test, degree)\n",
    "    \n",
    "    w_initial = np.array([0]*(phi_train.shape[1]))\n",
    "    max_iters = 1000\n",
    "\n",
    "    \n",
    "    # calcualte weight through least square.: TODO\n",
    "    w,loss_tr = logistic_regression(y_train, phi_train, w_initial, max_iters, gamma)\n",
    "    \n",
    "    loss_te = compute_logistic_loss(y_test, phi_test, w)\n",
    "    # compute error and loss for train and test data\n",
    "    error, ratio_error_train = compute_classification_error(y_train, phi_train, w)\n",
    "    error, ratio_error_test = compute_classification_error(y_test, phi_test, w)\n",
    "\n",
    "    \n",
    "    print(\"proportion={p}, degree={d}, gamma={g}, Train loss, err_ratio=({tr:.3f},{er_tr:.3f}), Test loss, err_ratio=({te:.3f},{er_te:.3f})\".format(\n",
    "          p=ratio, d=degree, g=gamma, tr=loss_tr, te=loss_te, er_tr=ratio_error_train, er_te=ratio_error_test))\n",
    "    return w, ratio_error_train, ratio_error_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 6\n",
    "degrees = [1, 3, 7, 12]\n",
    "split_ratios = [0.9, 0.8, 0.5, 0.1]\n",
    "gammas = [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.2, 0.5]\n",
    "\n",
    "ws = []\n",
    "ratio_err_trains = []\n",
    "ratio_err_tests = []\n",
    "\n",
    "for split_ratio in split_ratios:\n",
    "    for degree in degrees:\n",
    "        for gamma in gammas:\n",
    "            w, ratio_error_train, ratio_error_test = logistic_regression_split(x, y, degree, split_ratio, gamma, seed)\n",
    "            ws.append(w)\n",
    "            ratio_err_trains.append(ratio_error_train)\n",
    "            ratio_err_tests.append(ratio_error_test)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation with regularized logistic regression\n",
    "Voilà le code utilisé pour la première soumission. Attention: le code est long... pour bien faire il faudrait mettre un grand nombre de paramètres et faire tourner le code pendant ~1heure (brute-force, mais il se peut que ça fonctionne bien)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reg_logistic_regression_split(x, y, degree, ratio, gamma, lambda_, seed):\n",
    "    \"\"\"polynomial regression with different split ratios and different degrees.\"\"\"\n",
    "    # split the data, and return train and test data: TODO\n",
    "    x_train, x_test, y_train, y_test = split_data(x, y, ratio, seed)\n",
    "\n",
    "    # form train and test data with polynomial basis function: TODO\n",
    "    phi_train = build_poly(x_train, degree)\n",
    "    phi_test = build_poly(x_test, degree)\n",
    "    \n",
    "    w_initial = np.array([0]*(phi_train.shape[1]))\n",
    "    max_iters = 1000\n",
    "\n",
    "    \n",
    "    # calcualte weight through logistic regression\n",
    "    w, loss_tr = reg_logistic_regression(y_train, phi_train, lambda_, w_initial, max_iters, gamma, SGD=False)\n",
    "    \n",
    "    loss_te = compute_logistic_loss(y_test, phi_test, w)\n",
    "    # compute error and loss for train and test data\n",
    "    error, ratio_error_train = compute_classification_error(y_train, phi_train, w)\n",
    "    error, ratio_error_test = compute_classification_error(y_test, phi_test, w)\n",
    "\n",
    "    \n",
    "    print(\"prop={p}, deg={d}, g={g:.3f}, l={l:.3f}, Train loss, err_ratio=({tr:.3f},{er_tr:.3f}), Test loss, err_ratio=({te:.3f},{er_te:.3f})\".format(\n",
    "          p=ratio, d=degree, g=gamma, l=lambda_, tr=loss_tr, te=loss_te, er_tr=ratio_error_train, er_te=ratio_error_test))\n",
    "    return w, ratio_error_train, ratio_error_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 6\n",
    "degrees = [1, 7]\n",
    "split_ratios = [0.9]\n",
    "gammas = [1e-9, 1e-5, 1e-1, 0.5]\n",
    "lambdas = np.logspace(-5, 0, 2)\n",
    "\n",
    "ws = []\n",
    "ratio_err_trains = []\n",
    "ratio_err_tests = []\n",
    "\n",
    "for split_ratio in split_ratios:\n",
    "    for degree in degrees:\n",
    "        for gamma in gammas:\n",
    "            for lambda_ in lambdas:\n",
    "                w, ratio_error_train, ratio_error_test = reg_logistic_regression_split(x, y, degree, split_ratio, gamma, lambda_, seed)\n",
    "                ws.append(w)\n",
    "                ratio_err_trains.append(ratio_error_train)\n",
    "                ratio_err_tests.append(ratio_error_test)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_reg_log_reg = ws[np.argmin(ratio_err_tests)]\n",
    "np.argmin(ratio_err_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform estimation on evaluation set\n",
    "Une fois qu'on a le bon vecteur de poids, il suffit de runner ce code pour avoir le fichier de sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ukn = predict_labels(w_reg_log_reg, tx_ukn)\n",
    "y_ukn[y_ukn==0] = -1\n",
    "create_csv_submission(ids_ukn, y_ukn, 'submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
