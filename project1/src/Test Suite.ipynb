{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test suite for Project 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "from auxiliary import *\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "* `../data/train.csv`\n",
    "* `../data/test.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = '../data/train.csv'\n",
    "test_path  = '../data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Long, run only if necessary\n",
    "train_data = load_csv_data(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Long, run only if necessary\n",
    "test_data = load_csv_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the data as pickles for more efficient reloading\n",
    "# Only run once to generate the pickle !\n",
    "# pickle.dump(test_data, open( 'test.p', 'wb' ))\n",
    "# pickle.dump(train_data, open( 'train.p', 'wb' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading the pickle back \n",
    "# train_data = pickle.load(open( 'train.p', 'rb' ))\n",
    "# test_data = pickle.load(open( 'test.p', 'rb' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory\n",
    "\n",
    "* Describe / Discover the data\n",
    "* Describe / Characterise the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 250000 samples.\n"
     ]
    }
   ],
   "source": [
    "## Data dimensions\n",
    "print(\"There are {} samples.\".format(len(train_data[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is defined as a set $D = \\{ (y_i, \\mathbf{x}_i, w_i) \\}$ with :\n",
    "* $y_i \\in \\{+1,-1\\}$ is the label (signal = `+1` or noise = `-1`)\n",
    "* $\\mathbf{x}_i \\in \\!R^d$ is a $d$-dimensional feature vector\n",
    "* $w_i \\in \\!R^+$ is a non-negative weight\n",
    "\n",
    "Note that $\\sum_{i\\in\\mathcal{S}} w_i = N_s$ and $\\sum_{i\\in\\mathcal{B}} w_i = N_b$ which are the *expected total number of signal and background events (resp.)*. This gives an estimate of how many events we should expect to classify for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The feature vector shape is \n",
    "train_data[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which means we have d features :\n",
    "d = train_data[2].shape[1]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids = train_data[0]\n",
    "labels  = train_data[1]\n",
    "x_train = train_data[2]\n",
    "\n",
    "if(labels.ndim<2):\n",
    "    labels = np.expand_dims(labels, axis=1) # expand the labels as array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ids = train_data[0]\n",
    "test_labels  = train_data[1]\n",
    "test_x = train_data[2]\n",
    "\n",
    "if(test_labels.ndim<2):\n",
    "    test_labels = np.expand_dims(test_labels, axis=1) # expand the labels as array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information about the features :\n",
    "\n",
    "* Variables are floating point unless specified otherwise.\n",
    "* All azimuthal φ angles are in radian in the [−π, +π[ range.\n",
    "* Energy, mass, momentum are all in GeV\n",
    "* All other variables are unit less\n",
    "* **Undefined values are `-999.0`**\n",
    "\n",
    "There are `primitive` (prefixed with `PRI`) values, directly measured from the collision, and `derived` (prefixed with `DER`) values which were computed from the primitive values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 17\n"
     ]
    }
   ],
   "source": [
    "PRI_features = x_train[:,:13]\n",
    "DER_features = x_train[:,13:]\n",
    "print(len(PRI_features[0]), len(DER_features[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "* Split test / train\n",
    "* Clean useless features\n",
    "* [Scale / process data](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/) (e.g. scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much data is perfectly clean ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.245599999999996"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data = [x for x in x_train if not -999. in x]\n",
    "len(clean_data)/len(x_train)*100 # percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much data is clean in the primitive or derived classes ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.245599999999996"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRI_clean_data = [x for x in PRI_features if not -999. in x]\n",
    "len(PRI_clean_data)/len(PRI_features)*100 # percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.0172"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DER_clean_data = [x for x in DER_features if not -999. in x]\n",
    "len(DER_clean_data)/len(DER_features)*100 # percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage is almost the same ! This means there was not a lot of treatment error, using only `PRI` or only `DER` data won't make the data cleaner (but it might help reduce the complexity of the model ;) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there classes that contain a high probability of dirty data ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "per_feature_stats = []\n",
    "for i in range(d):\n",
    "    sz = len(x_train[:,i])\n",
    "    clean_ft = [x for x in x_train[:,i] if (x != -999.)] # c pa bo ... :'(\n",
    "    per_feature_stats.append(len(clean_ft) / sz * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  7.,   0.,   0.,   0.,   3.,   0.,   0.,   1.,   0.,  19.]),\n",
       " array([  29.0172 ,   36.11548,   43.21376,   50.31204,   57.41032,\n",
       "          64.5086 ,   71.60688,   78.70516,   85.80344,   92.90172,  100.     ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAFkCAYAAACq4KjhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2eiT3RwdfwcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117d000f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(per_feature_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a pretty good indication of the features that should be pruned from the data ! Notably :\n",
    "* `x_train[:, 4: 7]` (which are resp. `DER_deltaeta_jet_jet`, `DER_mass_jet_jet`, `DER_prodeta_jet_jet`)\n",
    "* `x_train[:,12]` (which is `DER_lep_eta_centrality`)\n",
    "* `x_train[:,26:29]` (which is the `PRI_jet_subleading_{pt,eta,phi}`)\n",
    "\n",
    "We can also consider removing `x_train[:,23:26]` (`PRI_jet_leading_{pt,eta,phi}`) which only has ~60% clean data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only 100% clean data\n",
    "ranges = [(1,4), (7,12), (13,23), (29,30)]\n",
    "keep_idx = build_idx(ranges)\n",
    "x_train_full_clean = x_train[:,keep_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With feature 1 and 23-26\n",
    "ranges = [(0,4), (7,12), (13,26), (29,30)]\n",
    "keep_idx = build_idx(ranges)\n",
    "x_train_partial_clean = x_train[:,keep_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With features 1-3\n",
    "ranges = [(1,4)]\n",
    "keep_idx = build_idx(ranges)\n",
    "x_train_small_features = x_train[:,keep_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the size of the sets, and expand the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(250000, 19)\n",
      "(250000, 23)\n",
      "(250000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train_full_clean.shape)\n",
    "print(x_train_partial_clean.shape)\n",
    "print(x_train_small_features.shape)\n",
    "\n",
    "tx_train = np.c_[np.ones((labels.shape[0], 1)), x_train]\n",
    "tx_train_full_clean = np.c_[np.ones((labels.shape[0], 1)), x_train_full_clean]\n",
    "tx_train_partial_clean = np.c_[np.ones((labels.shape[0], 1)), x_train_partial_clean]\n",
    "tx_train_small_features = np.c_[np.ones((labels.shape[0], 1)), x_train_small_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a very small set for testing the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Needs to be randomly sampled !\n",
    "sample_small_set = 1000\n",
    "\n",
    "small_tx_train = tx_train_full_clean[:sample_small_set, :]\n",
    "small_labels = labels[:sample_small_set].flatten()\n",
    "\n",
    "dim = small_tx_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "small_tx_std, mean_tx, std_tx = standardize(small_tx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38.333962050000004, 73.500211504812071, -5.6843418860808016e-17, 1.0)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_tx, std_tx, np.mean(small_tx_std), np.std(small_tx_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset_x_test_full_clean = test_x[sample_small_set:2*sample_small_set,:]\n",
    "subset_test_labels = test_labels[sample_small_set:2*sample_small_set].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "small_test_x_std, mean_test_x, std_test_x = standardize(small_tx_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "* **Train** the model with the different learning algorithms\n",
    "    * `least_squares`\n",
    "    * `least_squares_GD`\n",
    "    * `least_squares_SGD`\n",
    "    * `ridge_regression`\n",
    "    * `logistic_regression`\n",
    "    * `reg_logistic_regression`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grid search is much too long, least_squares sucks ...\n",
    "# grid_w = generate_w(dim=dim,num_intervals=10, upper=100, lower=-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least Squares with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_initial = np.array([0]*dim)\n",
    "max_iters = 100\n",
    "gammas = [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "losses = []\n",
    "ws = [] \n",
    "\n",
    "for gamma in gammas:\n",
    "    w, loss = least_squares_GD(small_labels, small_tx_std, w_initial, max_iters, gamma)\n",
    "    losses.append(loss)\n",
    "    ws.append(w)\n",
    "\n",
    "# TODO : Formally get best params. In this case 1e-9 is the best so we'll see later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5000000611911537,\n",
       " 0.50000061191767431,\n",
       " 0.5000061197905592,\n",
       " 0.50006125934938073,\n",
       " 0.50061880056390395,\n",
       " 0.50687675941004973,\n",
       " 0.82414554752788527,\n",
       " 187101287673.21201]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict a few values to check the test accuracy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method -- Least Squares GD\n",
      "params -- gamma = 1e-09, max_iters = 100\n",
      "\n",
      "On a simple subset we obtain 68.30 percent accuracy\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_classification_error(subset_test_labels, small_test_x_std, ws[0])\n",
    "print(\"method -- Least Squares GD\")\n",
    "print(\"params -- gamma = {}, max_iters = {}\".format(gammas[0], max_iters))\n",
    "print()\n",
    "print(\"On a simple subset we obtain %.2f percent accuracy\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### least_squares_SGD\n",
    "\n",
    "Comments : Same results as `least_square_GD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "gammas = [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "batch_size = 10\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0]*dim)\n",
    "losses = []\n",
    "ws = [] \n",
    "\n",
    "for gamma in gammas:\n",
    "    sgd_ws, sgd_losses = least_squares_SGD(small_labels, small_tx_std, w_initial, batch_size, max_iters, gamma)\n",
    "    losses.append(sgd_losses)\n",
    "    ws.append(sgd_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.50000012979479314,\n",
       " 0.50000015936171738,\n",
       " 0.50000256482307859,\n",
       " 0.50010230278493528,\n",
       " 0.50165870755937725,\n",
       " 0.52030280771213011,\n",
       " 0.95860061294089272,\n",
       " 51596308359.381584]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method -- Least Squares SGD\n",
      "params -- gamma = 1e-09, max_iters = 100\n",
      "\n",
      "On a simple subset we obtain 68.30 percent accuracy\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_classification_error(subset_test_labels, small_test_x_std, ws[0])\n",
    "print(\"method -- Least Squares SGD\")\n",
    "print(\"params -- gamma = {}, max_iters = {}\".format(gammas[0], max_iters))\n",
    "print()\n",
    "print(\"On a simple subset we obtain %.2f percent accuracy\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ridge_regression\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00000, Training RMSE=1.185, Testing RMSE=1.498\n",
      "lambda=0.00000, Training RMSE=1.185, Testing RMSE=1.498\n",
      "lambda=0.00000, Training RMSE=1.185, Testing RMSE=1.498\n",
      "lambda=0.00000, Training RMSE=1.185, Testing RMSE=1.498\n",
      "lambda=0.00000, Training RMSE=1.185, Testing RMSE=1.497\n",
      "lambda=0.00000, Training RMSE=1.185, Testing RMSE=1.496\n",
      "lambda=0.00000, Training RMSE=1.186, Testing RMSE=1.489\n",
      "lambda=0.00001, Training RMSE=1.193, Testing RMSE=1.471\n",
      "lambda=0.00005, Training RMSE=1.211, Testing RMSE=1.449\n",
      "lambda=0.00027, Training RMSE=1.229, Testing RMSE=1.433\n",
      "lambda=0.00139, Training RMSE=1.238, Testing RMSE=1.424\n",
      "lambda=0.00720, Training RMSE=1.242, Testing RMSE=1.412\n",
      "lambda=0.03728, Training RMSE=1.252, Testing RMSE=1.380\n",
      "lambda=0.19307, Training RMSE=1.287, Testing RMSE=1.343\n",
      "lambda=1.00000, Training RMSE=1.332, Testing RMSE=1.339\n"
     ]
    }
   ],
   "source": [
    "lambdas = np.logspace(-10, 0, 15)\n",
    "\n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "\n",
    "ws = []\n",
    "    \n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    weights, loss = ridge_regression(small_labels, small_tx_std, lambda_)\n",
    "    \n",
    "    ws.append(weights)\n",
    "    \n",
    "    pred_train = small_tx_std.dot(weights)\n",
    "    pred_test  = small_test_x_std.dot(weights)\n",
    "\n",
    "    rmse_tr.append(np.sqrt(2*((pred_train-small_labels)**2).mean()))\n",
    "    rmse_te.append(np.sqrt(2*((pred_test -subset_test_labels)**2) .mean()))\n",
    "        \n",
    "    print(\"lambda={l:.5f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "           l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method -- Ridge regression\n",
      "params -- lambda = 0.0002682695795279727, max_iters = 100\n",
      "\n",
      "On a simple subset we obtain 40.80 percent accuracy\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_classification_error(subset_test_labels, small_test_x_std, ws[9])\n",
    "print(\"method -- Ridge regression\")\n",
    "print(\"params -- lambda = {}, max_iters = {}\".format(lambdas[9], max_iters))\n",
    "print()\n",
    "print(\"On a simple subset we obtain %.2f percent accuracy\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great accuracy at all ... Let's check with some polynomial dimensions added :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=10, lambda=0.00000, Training RMSE=1.073, Testing RMSE=1.634\n",
      "degree=10, lambda=0.00000, Training RMSE=1.035, Testing RMSE=1.604\n",
      "degree=10, lambda=0.00000, Training RMSE=1.033, Testing RMSE=1.602\n",
      "degree=10, lambda=0.00000, Training RMSE=1.060, Testing RMSE=1.619\n",
      "degree=10, lambda=0.00000, Training RMSE=1.166, Testing RMSE=1.668\n",
      "degree=10, lambda=0.00000, Training RMSE=1.044, Testing RMSE=1.589\n",
      "degree=10, lambda=0.00000, Training RMSE=1.073, Testing RMSE=1.620\n",
      "degree=10, lambda=0.00001, Training RMSE=1.053, Testing RMSE=1.575\n",
      "degree=10, lambda=0.00005, Training RMSE=1.051, Testing RMSE=1.580\n",
      "degree=10, lambda=0.00027, Training RMSE=1.053, Testing RMSE=1.576\n",
      "degree=10, lambda=0.00139, Training RMSE=1.059, Testing RMSE=1.569\n",
      "degree=10, lambda=0.00720, Training RMSE=1.067, Testing RMSE=1.563\n",
      "degree=10, lambda=0.03728, Training RMSE=1.078, Testing RMSE=1.555\n",
      "degree=10, lambda=0.19307, Training RMSE=1.094, Testing RMSE=1.542\n",
      "degree=10, lambda=1.00000, Training RMSE=1.109, Testing RMSE=1.526\n"
     ]
    }
   ],
   "source": [
    "lambdas = np.logspace(-10, 0, 15)\n",
    "degree = 10\n",
    "\n",
    "poly_train = np.asarray(build_poly(small_tx_std, degree))\n",
    "poly_test  = np.asarray(build_poly(small_test_x_std, degree))\n",
    "\n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "\n",
    "ws = []\n",
    "\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    weights, loss = ridge_regression(small_labels, poly_train, lambda_)\n",
    "    \n",
    "    ws.append(weights)\n",
    "    \n",
    "    pred_train = poly_train.dot(weights)\n",
    "    pred_test  = poly_test.dot(weights)\n",
    "\n",
    "    rmse_tr.append(np.sqrt(2*((pred_train-small_labels)**2).mean()))\n",
    "    rmse_te.append(np.sqrt(2*((pred_test -subset_test_labels)**2).mean()))\n",
    "        \n",
    "    print(\"degree={d}, lambda={l:.5f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "           d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method -- Ridge regression with poly\n",
      "params -- lambda = 2.6826957952797275e-09, degree = 10\n",
      "\n",
      "On a simple subset we obtain 45.30 percent accuracy\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_classification_error(subset_test_labels, poly_test, ws[2])\n",
    "print(\"method -- Ridge regression with poly\")\n",
    "print(\"params -- lambda = {}, degree = {}\".format(lambdas[2], degree))\n",
    "print()\n",
    "print(\"On a simple subset we obtain %.2f percent accuracy\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### logistic_regression\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reg_logistic_regression\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO Check impl, there's a problem here\n",
    "def cross_validation(y, tx, k_indices, k, lambda_):\n",
    "    \"\"\"return the loss of logistic regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    k_indices = k_indices.astype(int)\n",
    "    k_indices_train = np.array([])\n",
    "    \n",
    "    for i in range(k_indices.shape[0]):\n",
    "        if i != k:\n",
    "            k_indices_train = np.append(k_indices_train, k_indices[i])\n",
    "    \n",
    "    k_indices_test = k_indices[k]\n",
    "    \n",
    "    tx_train = tx[k_indices_train.astype(int)]\n",
    "    y_train = y[k_indices_train.astype(int)]\n",
    "    \n",
    "    tx_test = tx[k_indices_test.astype(int)]\n",
    "    y_test = y[k_indices_test.astype(int)]\n",
    "\n",
    "    # logistic regression\n",
    "    w = logistic_regression_gradient_descent(y_train, tx_train, lambda_)\n",
    "    \n",
    "    # calculate the loss for train and test data\n",
    "    loss_tr = compute_classification_error(y_train,tx_train,w)\n",
    "    loss_te = compute_classification_error(y_test,tx_test,w)\n",
    "\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plots import cross_validation_visualization\n",
    "# TODO Check impl, there's a problem here\n",
    "\n",
    "def cross_validation_demo(y, x):\n",
    "    verbose = True\n",
    "    seed = 1\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-4, 0, 30)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    print(\"Current run: {N} samples, {f} features\".format(N=x.shape[0], f=x.shape[1]))\n",
    "\n",
    "\n",
    "    # define lists to store the loss of training data and test data\n",
    "    final_losses_tr = []\n",
    "    final_losses_te = []\n",
    "\n",
    "    # cross validation: TODO\n",
    "    for idx, lambda_ in enumerate(lambdas):\n",
    "        losses_tr = []\n",
    "        losses_te = []\n",
    "        for k in range(k_fold):\n",
    "            loss_tr, loss_te = cross_validation(y, x, k_indices, k, lambda_)\n",
    "            losses_tr.append(loss_tr)\n",
    "            losses_te.append(loss_te)\n",
    "        final_losses_tr.append(np.mean(losses_tr))\n",
    "        final_losses_te.append(np.mean(losses_te))\n",
    "        if verbose:\n",
    "            print(\"Current lambda: {i} out of {j}\".format(i=idx, j=len(lambdas)))\n",
    "    \n",
    "    cross_validation_visualization(lambdas, final_losses_tr, final_losses_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current run: 1000 samples, 20 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Dylan/Documents/Cours/MT_MA3/Machine Learning/ML_Projects/project1/src/auxiliary.py:20: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t reg_logistic_regression: stop due to max_iters\n",
      "\t reg_logistic_regression: stop due to max_iters\n",
      "\t reg_logistic_regression: stop due to max_iters\n",
      "\t reg_logistic_regression: stop due to max_iters\n",
      "Current lambda: 0 out of 30\n",
      "\t reg_logistic_regression: stop due to max_iters\n",
      "\t reg_logistic_regression: stop due to max_iters\n",
      "\t reg_logistic_regression: stop due to max_iters\n",
      "\t reg_logistic_regression: stop due to max_iters\n",
      "Current lambda: 1 out of 30\n",
      "\t reg_logistic_regression: stop due to max_iters\n",
      "\t reg_logistic_regression: stop due to max_iters\n",
      "\t reg_logistic_regression: stop due to max_iters\n",
      "\t reg_logistic_regression: stop due to max_iters\n",
      "Current lambda: 2 out of 30\n",
      "\t reg_logistic_regression: stop due to max_iters\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-f6a65a8942fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmall_tx_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmall_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcross_validation_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-230e60d78521>\u001b[0m in \u001b[0;36mcross_validation_demo\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mlosses_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mloss_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mlosses_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mlosses_te\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-291762014514>\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(y, tx, k_indices, k, lambda_)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# logistic regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# calculate the loss for train and test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-6fbde3d230af>\u001b[0m in \u001b[0;36mlogistic_regression_gradient_descent\u001b[0;34m(y, tx, lambda_)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mw_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Dylan/Documents/Cours/MT_MA3/Machine Learning/ML_Projects/project1/src/implementations.py\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[0;34m(y, tx, lambda_, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_logistic_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m#print('compute gradient')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_logistic_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;31m#print('compute regularizers')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mloss_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Dylan/Documents/Cours/MT_MA3/Machine Learning/ML_Projects/project1/src/auxiliary.py\u001b[0m in \u001b[0;36mcompute_logistic_gradient\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_logistic_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Dylan/Documents/Cours/MT_MA3/Machine Learning/ML_Projects/project1/src/auxiliary.py\u001b[0m in \u001b[0;36msigmoid\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_logistic_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tx = small_tx_train\n",
    "y = small_labels\n",
    "cross_validation_demo(y,tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "* **Test** the model with the weights computed from the different learning algorithms to find the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validate\n",
    "\n",
    "* the hyperparameters for each algorithm\n",
    "    * `least_squares` \n",
    "    * `least_squares_GD` -> `gamma`\n",
    "    * `least_squares_SGD` -> `gamma`, `batch_size`\n",
    "    * `ridge_regression` -> `lambda_`\n",
    "    * `logistic_regression` -> `gamma`\n",
    "    * `reg_logistic_regression` -> `lambda_`, `gamma`\n",
    "* Also, if we use other features (e.g. polynomial), we should CV those as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "* Plot the train / test accuracies for the best set of algorithm + parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
