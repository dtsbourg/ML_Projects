{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test suite for Project 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "* `../data/train.csv`\n",
    "* `../data/test.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = '../data/train.csv'\n",
    "test_path  = '../data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Long, run only if necessary\n",
    "train_data = load_csv_data(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Long, run only if necessary\n",
    "test_data = load_csv_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the data as pickles for more efficient reloading\n",
    "# Only run once to generate the pickle !\n",
    "# pickle.dump(test_data, open( 'test.p', 'wb' ))\n",
    "# pickle.dump(train_data, open( 'train.p', 'wb' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loading the pickle back \n",
    "# train_data = pickle.load(open( 'train.p', 'rb' ))\n",
    "# test_data = pickle.load(open( 'test.p', 'rb' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory\n",
    "\n",
    "* Describe / Discover the data\n",
    "* Describe / Characterise the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 250000 samples.\n"
     ]
    }
   ],
   "source": [
    "## Data dimensions\n",
    "print(\"There are {} samples.\".format(len(train_data[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is defined as a set $D = \\{ (y_i, \\mathbf{x}_i, w_i) \\}$ with :\n",
    "* $y_i \\in \\{+1,-1\\}$ is the label (signal = `+1` or noise = `-1`)\n",
    "* $\\mathbf{x}_i \\in \\!R^d$ is a $d$-dimensional feature vector\n",
    "* $w_i \\in \\!R^+$ is a non-negative weight\n",
    "\n",
    "Note that $\\sum_{i\\in\\mathcal{S}} w_i = N_s$ and $\\sum_{i\\in\\mathcal{B}} w_i = N_b$ which are the *expected total number of signal and background events (resp.)*. This gives an estimate of how many events we should expect to classify for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The feature vector shape is \n",
    "train_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which means we have d features :\n",
    "d = train_data[1].shape[1]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels  = train_data[0]\n",
    "x_train = train_data[1]\n",
    "weights = train_data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information about the features :\n",
    "\n",
    "* Variables are floating point unless specified otherwise.\n",
    "* All azimuthal φ angles are in radian in the [−π, +π[ range.\n",
    "* Energy, mass, momentum are all in GeV\n",
    "* All other variables are unit less\n",
    "* **Undefined values are `-999.0`**\n",
    "\n",
    "There are `primitive` (prefixed with `PRI`) values, directly measured from the collision, and `derived` (prefixed with `DER`) values which were computed from the primitive values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250000 250000\n"
     ]
    }
   ],
   "source": [
    "PRI_features = x_train[:,1:15]\n",
    "DER_features = x_train[:,15:]\n",
    "print(len(DER_features), len(PRI_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "* Split test / train\n",
    "* Clean useless features\n",
    "* [Scale / process data](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/) (e.g. scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much data is perfectly clean ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.245599999999996"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data = [x for x in x_train if not -999. in x]\n",
    "len(clean_data)/len(x_train)*100 # percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much data is clean in the primitive or derived classes ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.0172"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRI_clean_data = [x for x in PRI_features if not -999. in x]\n",
    "len(PRI_clean_data)/len(PRI_features)*100 # percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.0172"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DER_clean_data = [x for x in DER_features if not -999. in x]\n",
    "len(DER_clean_data)/len(DER_features)*100 # percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage is the same ! This means there was no treatment error, using only `PRI` or only `DER` data won't make the data cleaner (but it might help reduce the complexity of the model ;) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there classes that contain a high probability of dirty data ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "per_feature_stats = []\n",
    "for i in range(d):\n",
    "    sz = len(x_train[:,i])\n",
    "    clean_ft = [x for x in x_train[:,i] if (x != -999.)] # c pa bo ... :'(\n",
    "    per_feature_stats.append(len(clean_ft) / sz * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[84.75439999999999,\n",
       " 100.0,\n",
       " 100.0,\n",
       " 100.0,\n",
       " 29.0172,\n",
       " 29.0172,\n",
       " 29.0172,\n",
       " 100.0,\n",
       " 100.0,\n",
       " 100.0,\n",
       " 100.0,\n",
       " 100.0,\n",
       " 29.0172,\n",
       " 100.0,\n",
       " 100.0,\n",
       " 100.0,\n",
       " 100.0,\n",
       " 100.0,\n",
       " 100.0,\n",
       " 100.0,\n",
       " 100.0,\n",
       " 100.0,\n",
       " 100.0,\n",
       " 60.0348,\n",
       " 60.0348,\n",
       " 60.0348,\n",
       " 29.0172,\n",
       " 29.0172,\n",
       " 29.0172,\n",
       " 100.0]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_feature_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a pretty good indication of the features that should be pruned from the data ! Notably :\n",
    "* `x_train[:, 4: 7]` (which are resp. `DER_deltaeta_jet_jet`, `DER_mass_jet_jet`, `DER_prodeta_jet_jet`)\n",
    "* `x_train[:,12]` (which is `DER_lep_eta_centrality`)\n",
    "* `x_train[:,26:29]` (which is the `PRI_jet_subleading_{pt,eta,phi}`)\n",
    "\n",
    "We can also consider removing `x_train[:,23:26]` (`PRI_jet_leading_{pt,eta,phi}`) which only has ~60% clean data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_idx(ranges):\n",
    "    clean_idx = np.asarray([list(range(i,j)) for i,j in ranges])\n",
    "    return [item for sublist in clean_idx for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only 100% clean data\n",
    "ranges = [(1,4), (7,12), (13,26), (29,30)]\n",
    "keep_idx = build_idx(ranges)\n",
    "x_train_full_clean = x_train[:,keep_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With feature 1\n",
    "ranges = [(0,4), (7,12), (13,26), (29,30)]\n",
    "keep_idx = build_idx(ranges)\n",
    "x_train_partial_clean = x_train[:,keep_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "* **Train** the model with the different learning algorithms\n",
    "    * `least_squares`\n",
    "    * `least_squares_GD`\n",
    "    * `least_squares_SGD`\n",
    "    * `ridge_regression`\n",
    "    * `logistic_regression`\n",
    "    * `reg_logistic_regression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "* **Test** the model with the weights computed from the different learning algorithms to find the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validate\n",
    "\n",
    "* the hyperparameters for each algorithm\n",
    "    * `least_squares` \n",
    "    * `least_squares_GD` -> `gamma`\n",
    "    * `least_squares_SGD` -> `gamma`, `batch_size`\n",
    "    * `ridge_regression` -> `lambda_`\n",
    "    * `logistic_regression` -> `gamma`\n",
    "    * `reg_logistic_regression` -> `lambda_`, `gamma`\n",
    "* Also, if we use other features (e.g. polynomial), we should CV those as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "* Plot the train / test accuracies for the best set of algorithm + parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
