{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test suite for Project 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "from auxiliary import *\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "* `../data/train.csv`\n",
    "* `../data/test.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = '../data/train.csv'\n",
    "test_path  = '../data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Long, run only if necessary\n",
    "# train_data = load_csv_data(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Long, run only if necessary\n",
    "# test_data = load_csv_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the data as pickles for more efficient reloading\n",
    "# Only run once to generate the pickle !\n",
    "# pickle.dump(test_data, open( 'test.p', 'wb' ))\n",
    "# pickle.dump(train_data, open( 'train.p', 'wb' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading the pickle back \n",
    "train_data = pickle.load(open( 'train.p', 'rb' ))\n",
    "test_data = pickle.load(open( 'test.p', 'rb' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory\n",
    "\n",
    "* Describe / Discover the data\n",
    "* Describe / Characterise the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 250000 samples.\n"
     ]
    }
   ],
   "source": [
    "## Data dimensions\n",
    "print(\"There are {} samples.\".format(len(train_data[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is defined as a set $D = \\{ (y_i, \\mathbf{x}_i, w_i) \\}$ with :\n",
    "* $y_i \\in \\{+1,-1\\}$ is the label (signal = `+1` or noise = `-1`)\n",
    "* $\\mathbf{x}_i \\in \\!R^d$ is a $d$-dimensional feature vector\n",
    "* $w_i \\in \\!R^+$ is a non-negative weight\n",
    "\n",
    "Note that $\\sum_{i\\in\\mathcal{S}} w_i = N_s$ and $\\sum_{i\\in\\mathcal{B}} w_i = N_b$ which are the *expected total number of signal and background events (resp.)*. This gives an estimate of how many events we should expect to classify for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The feature vector shape is \n",
    "train_data[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which means we have d features :\n",
    "d = train_data[2].shape[1]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids = train_data[0]\n",
    "labels  = train_data[1]\n",
    "x_train = train_data[2]\n",
    "\n",
    "if(labels.ndim<2):\n",
    "    labels = np.expand_dims(labels, axis=1) # expand the labels as array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ids = train_data[0]\n",
    "test_labels  = train_data[1]\n",
    "test_x = train_data[2]\n",
    "\n",
    "if(test_labels.ndim<2):\n",
    "    test_labels = np.expand_dims(test_labels, axis=1) # expand the labels as array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information about the features :\n",
    "\n",
    "* Variables are floating point unless specified otherwise.\n",
    "* All azimuthal φ angles are in radian in the [−π, +π[ range.\n",
    "* Energy, mass, momentum are all in GeV\n",
    "* All other variables are unit less\n",
    "* **Undefined values are `-999.0`**\n",
    "\n",
    "There are `primitive` (prefixed with `PRI`) values, directly measured from the collision, and `derived` (prefixed with `DER`) values which were computed from the primitive values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 17\n"
     ]
    }
   ],
   "source": [
    "PRI_features = x_train[:,:13]\n",
    "DER_features = x_train[:,13:]\n",
    "print(len(PRI_features[0]), len(DER_features[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "* Split test / train\n",
    "* Clean useless features\n",
    "* [Scale / process data](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/) (e.g. scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much data is perfectly clean ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.245599999999996"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data = [x for x in x_train if not -999. in x]\n",
    "len(clean_data)/len(x_train)*100 # percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much data is clean in the primitive or derived classes ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.245599999999996"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRI_clean_data = [x for x in PRI_features if not -999. in x]\n",
    "len(PRI_clean_data)/len(PRI_features)*100 # percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.0172"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DER_clean_data = [x for x in DER_features if not -999. in x]\n",
    "len(DER_clean_data)/len(DER_features)*100 # percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage is almost the same ! This means there was not a lot of treatment error, using only `PRI` or only `DER` data won't make the data cleaner (but it might help reduce the complexity of the model ;) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there classes that contain a high probability of dirty data ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "per_feature_stats = []\n",
    "for i in range(d):\n",
    "    sz = len(x_train[:,i])\n",
    "    clean_ft = [x for x in x_train[:,i] if (x != -999.)] # c pa bo ... :'(\n",
    "    per_feature_stats.append(len(clean_ft) / sz * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  7.,   0.,   0.,   0.,   3.,   0.,   0.,   1.,   0.,  19.]),\n",
       " array([  29.0172 ,   36.11548,   43.21376,   50.31204,   57.41032,\n",
       "          64.5086 ,   71.60688,   78.70516,   85.80344,   92.90172,  100.     ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfklqjMEvSY0x+CWpMQa/JDXG4JekxvwvM2hP1ILGNHIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f1f87b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(per_feature_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a pretty good indication of the features that should be pruned from the data ! Notably :\n",
    "* `x_train[:, 4: 7]` (which are resp. `DER_deltaeta_jet_jet`, `DER_mass_jet_jet`, `DER_prodeta_jet_jet`)\n",
    "* `x_train[:,12]` (which is `DER_lep_eta_centrality`)\n",
    "* `x_train[:,26:29]` (which is the `PRI_jet_subleading_{pt,eta,phi}`)\n",
    "\n",
    "We can also consider removing `x_train[:,23:26]` (`PRI_jet_leading_{pt,eta,phi}`) which only has ~60% clean data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only 100% clean data\n",
    "ranges = [(1,4), (7,12), (13,23), (29,30)]\n",
    "keep_idx = build_idx(ranges)\n",
    "x_train_full_clean = x_train[:,keep_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With feature 1 and 23-26\n",
    "ranges = [(0,4), (7,12), (13,26), (29,30)]\n",
    "keep_idx = build_idx(ranges)\n",
    "x_train_partial_clean = x_train[:,keep_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With features 1-3\n",
    "ranges = [(1,4)]\n",
    "keep_idx = build_idx(ranges)\n",
    "x_train_small_features = x_train[:,keep_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the size of the sets, and expand the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(250000, 19)\n",
      "(250000, 23)\n",
      "(250000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train_full_clean.shape)\n",
    "print(x_train_partial_clean.shape)\n",
    "print(x_train_small_features.shape)\n",
    "\n",
    "tx_train = np.c_[np.ones((labels.shape[0], 1)), x_train]\n",
    "tx_train_full_clean = np.c_[np.ones((labels.shape[0], 1)), x_train_full_clean]\n",
    "tx_train_partial_clean = np.c_[np.ones((labels.shape[0], 1)), x_train_partial_clean]\n",
    "tx_train_small_features = np.c_[np.ones((labels.shape[0], 1)), x_train_small_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a very small set for testing the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs to be randomly sampled !\n",
    "sample_small_set = 1000\n",
    "\n",
    "small_tx_train = tx_train_full_clean[:sample_small_set, :]\n",
    "small_labels = labels[:sample_small_set].flatten()\n",
    "\n",
    "dim = small_tx_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_tx_std, mean_tx, std_tx = standardize(small_tx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38.333962050000004, 73.500211504812071, -5.6843418860808016e-17, 1.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_tx, std_tx, np.mean(small_tx_std), np.std(small_tx_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_x_test_full_clean = test_x[sample_small_set:2*sample_small_set,:]\n",
    "subset_test_labels = test_labels[sample_small_set:2*sample_small_set].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_test_x_std, mean_test_x, std_test_x = standardize(small_tx_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "* **Train** the model with the different learning algorithms\n",
    "    * `least_squares`\n",
    "    * `least_squares_GD`\n",
    "    * `least_squares_SGD`\n",
    "    * `ridge_regression`\n",
    "    * `logistic_regression`\n",
    "    * `reg_logistic_regression`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search is much too long, least_squares sucks ...\n",
    "# grid_w = generate_w(dim=dim,num_intervals=10, upper=100, lower=-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least Squares with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_initial = np.array([0]*dim)\n",
    "max_iters = 100\n",
    "gammas = [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "losses = []\n",
    "ws = [] \n",
    "\n",
    "for gamma in gammas:\n",
    "    w, loss = least_squares_GD(small_labels, small_tx_std, w_initial, max_iters, gamma)\n",
    "    losses.append(loss)\n",
    "    ws.append(w)\n",
    "\n",
    "# TODO : Formally get best params. In this case 1e-9 is the best so we'll see later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5000000611911537,\n",
       " 0.50000061191767431,\n",
       " 0.5000061197905592,\n",
       " 0.50006125934938073,\n",
       " 0.50061880056390395,\n",
       " 0.50687675941004973,\n",
       " 0.82414554752788527,\n",
       " 187101287673.21201]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict a few values to check the test accuracy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method -- Least Squares GD\n",
      "params -- gamma = 1e-09, max_iters = 100\n",
      "\n",
      "On a simple subset we obtain 68.30 percent accuracy\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_classification_error(subset_test_labels, small_test_x_std, ws[0])\n",
    "print(\"method -- Least Squares GD\")\n",
    "print(\"params -- gamma = {}, max_iters = {}\".format(gammas[0], max_iters))\n",
    "print()\n",
    "print(\"On a simple subset we obtain %.2f percent accuracy\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### least_squares_SGD\n",
    "\n",
    "Comments : Same results as `least_square_GD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "gammas = [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "batch_size = 10\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0]*dim)\n",
    "losses = []\n",
    "ws = [] \n",
    "\n",
    "for gamma in gammas:\n",
    "    sgd_ws, sgd_losses = least_squares_SGD(small_labels, small_tx_std, w_initial, batch_size, max_iters, gamma)\n",
    "    losses.append(sgd_losses)\n",
    "    ws.append(sgd_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.50000001775348402,\n",
       " 0.50000125776247073,\n",
       " 0.5000040020679587,\n",
       " 0.499977591712924,\n",
       " 0.50146387419735938,\n",
       " 0.50832680935947838,\n",
       " 0.67174639127301439,\n",
       " 281669959858.01428]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method -- Least Squares SGD\n",
      "params -- gamma = 1e-09, max_iters = 100\n",
      "\n",
      "On a simple subset we obtain 68.30 percent accuracy\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_classification_error(subset_test_labels, small_test_x_std, ws[0])\n",
    "print(\"method -- Least Squares SGD\")\n",
    "print(\"params -- gamma = {}, max_iters = {}\".format(gammas[0], max_iters))\n",
    "print()\n",
    "print(\"On a simple subset we obtain %.2f percent accuracy\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ridge_regression\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00000, Training RMSE=1.185, Testing RMSE=1.498\n",
      "lambda=0.00000, Training RMSE=1.185, Testing RMSE=1.498\n",
      "lambda=0.00000, Training RMSE=1.185, Testing RMSE=1.498\n",
      "lambda=0.00000, Training RMSE=1.185, Testing RMSE=1.498\n",
      "lambda=0.00000, Training RMSE=1.185, Testing RMSE=1.498\n",
      "lambda=0.00000, Training RMSE=1.185, Testing RMSE=1.498\n",
      "lambda=0.00000, Training RMSE=1.185, Testing RMSE=1.498\n",
      "lambda=0.00001, Training RMSE=1.185, Testing RMSE=1.498\n",
      "lambda=0.00005, Training RMSE=1.185, Testing RMSE=1.498\n",
      "lambda=0.00027, Training RMSE=1.185, Testing RMSE=1.497\n",
      "lambda=0.00139, Training RMSE=1.185, Testing RMSE=1.494\n",
      "lambda=0.00720, Training RMSE=1.187, Testing RMSE=1.483\n",
      "lambda=0.03728, Training RMSE=1.199, Testing RMSE=1.463\n",
      "lambda=0.19307, Training RMSE=1.218, Testing RMSE=1.442\n",
      "lambda=1.00000, Training RMSE=1.233, Testing RMSE=1.429\n"
     ]
    }
   ],
   "source": [
    "lambdas = np.logspace(-10, 0, 15)\n",
    "\n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "\n",
    "ws = []\n",
    "    \n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    weights, loss = ridge_regression(small_labels, small_tx_std, lambda_)\n",
    "    \n",
    "    ws.append(weights)\n",
    "    \n",
    "    pred_train = small_tx_std.dot(weights)\n",
    "    pred_test  = small_test_x_std.dot(weights)\n",
    "\n",
    "    rmse_tr.append(np.sqrt(2*((pred_train-small_labels)**2).mean()))\n",
    "    rmse_te.append(np.sqrt(2*((pred_test -subset_test_labels)**2) .mean()))\n",
    "        \n",
    "    print(\"lambda={l:.5f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "           l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method -- Ridge regression\n",
      "params -- lambda = 0.0002682695795279727, max_iters = 100\n",
      "\n",
      "On a simple subset we obtain 42.80 percent accuracy\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_classification_error(subset_test_labels, small_test_x_std, ws[9])\n",
    "print(\"method -- Ridge regression\")\n",
    "print(\"params -- lambda = {}, max_iters = {}\".format(lambdas[9], max_iters))\n",
    "print()\n",
    "print(\"On a simple subset we obtain %.2f percent accuracy\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great accuracy at all ... Let's check with some polynomial dimensions added :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=10, lambda=0.00000, Training RMSE=1.083, Testing RMSE=1.649\n",
      "degree=10, lambda=0.00000, Training RMSE=1.032, Testing RMSE=1.615\n",
      "degree=10, lambda=0.00000, Training RMSE=1.032, Testing RMSE=1.608\n",
      "degree=10, lambda=0.00000, Training RMSE=1.059, Testing RMSE=1.628\n",
      "degree=10, lambda=0.00000, Training RMSE=1.176, Testing RMSE=1.688\n",
      "degree=10, lambda=0.00000, Training RMSE=1.043, Testing RMSE=1.587\n",
      "degree=10, lambda=0.00000, Training RMSE=1.076, Testing RMSE=1.585\n",
      "degree=10, lambda=0.00001, Training RMSE=1.049, Testing RMSE=1.575\n",
      "degree=10, lambda=0.00005, Training RMSE=1.050, Testing RMSE=1.581\n",
      "degree=10, lambda=0.00027, Training RMSE=1.053, Testing RMSE=1.582\n",
      "degree=10, lambda=0.00139, Training RMSE=1.058, Testing RMSE=1.575\n",
      "degree=10, lambda=0.00720, Training RMSE=1.067, Testing RMSE=1.565\n",
      "degree=10, lambda=0.03728, Training RMSE=1.078, Testing RMSE=1.556\n",
      "degree=10, lambda=0.19307, Training RMSE=1.094, Testing RMSE=1.543\n",
      "degree=10, lambda=1.00000, Training RMSE=1.109, Testing RMSE=1.526\n"
     ]
    }
   ],
   "source": [
    "lambdas = np.logspace(-10, 0, 15)\n",
    "degree = 10\n",
    "\n",
    "poly_train = np.asarray(build_poly(small_tx_std, degree))\n",
    "poly_test  = np.asarray(build_poly(small_test_x_std, degree))\n",
    "\n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "\n",
    "ws = []\n",
    "\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    weights, loss = ridge_regression(small_labels, poly_train, lambda_)\n",
    "    \n",
    "    ws.append(weights)\n",
    "    \n",
    "    pred_train = poly_train.dot(weights)\n",
    "    pred_test  = poly_test.dot(weights)\n",
    "\n",
    "    rmse_tr.append(np.sqrt(2*((pred_train-small_labels)**2).mean()))\n",
    "    rmse_te.append(np.sqrt(2*((pred_test -subset_test_labels)**2).mean()))\n",
    "        \n",
    "    print(\"degree={d}, lambda={l:.5f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "           d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method -- Ridge regression with poly\n",
      "params -- lambda = 2.6826957952797275e-09, degree = 10\n",
      "\n",
      "On a simple subset we obtain 44.40 percent accuracy\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_classification_error(subset_test_labels, poly_test, ws[2])\n",
    "print(\"method -- Ridge regression with poly\")\n",
    "print(\"params -- lambda = {}, degree = {}\".format(lambdas[2], degree))\n",
    "print()\n",
    "print(\"On a simple subset we obtain %.2f percent accuracy\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### logistic_regression\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[692.99324698936471,\n",
       " 691.64598179293239,\n",
       " 681.1315590619995,\n",
       " 645.45256120492422,\n",
       " 599.40948952183362,\n",
       " 565.63552751785824,\n",
       " 772.93314659506916,\n",
       " 6391.4632590715582]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_initial = np.array([0]*dim)\n",
    "max_iters = 1000\n",
    "gammas = [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "losses = []\n",
    "ws = []\n",
    "\n",
    "small_labels_logistic =  np.copy(small_labels)\n",
    "small_labels_logistic[small_labels_logistic == -1] = 0\n",
    "\n",
    "for gamma in gammas:\n",
    "    w, loss = logistic_regression(small_labels_logistic, small_tx_std, w_initial, max_iters, gamma) \n",
    "    losses.append(loss)\n",
    "    ws.append(w)\n",
    "\n",
    "# TODO : Formally get best params. In this case 1e-9 is the best so we'll see later\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method -- Least Squares GD\n",
      "params -- gamma = 1e-09, max_iters = 1000\n",
      "\n",
      "On a simple subset we obtain 40.60 percent accuracy\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_classification_error(subset_test_labels, small_test_x_std, ws[5])\n",
    "print(\"method -- Least Squares GD\")\n",
    "print(\"params -- gamma = {}, max_iters = {}\".format(gammas[0], max_iters))\n",
    "print()\n",
    "print(\"On a simple subset we obtain %.2f percent accuracy\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a great accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reg_logistic_regression\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  692.99324699,   692.99324699,   692.99324699,   692.99324699,\n",
       "          692.99324699,   692.99324699,   692.99324699,   692.99324699],\n",
       "       [  691.64598179,   691.64598179,   691.64598179,   691.64598179,\n",
       "          691.64598179,   691.6459818 ,   691.64598182,   691.64598209],\n",
       "       [  681.13155906,   681.13155906,   681.13155906,   681.13155906,\n",
       "          681.13155908,   681.13155928,   681.13156126,   681.13158106],\n",
       "       [  645.4525612 ,   645.45256121,   645.45256121,   645.45256127,\n",
       "          645.45256187,   645.45256783,   645.45262745,   645.45322367],\n",
       "       [  599.40948952,   599.40948953,   599.4094896 ,   599.40949032,\n",
       "          599.40949747,   599.40956902,   599.41028448,   599.41743833],\n",
       "       [  565.63552752,   565.63552757,   565.63552799,   565.63553227,\n",
       "          565.63557505,   565.63600283,   565.64028034,   565.68302364],\n",
       "       [  772.93314666,   772.93314721,   772.93315278,   772.93320847,\n",
       "          772.93376535,   772.93933406,   772.99501138,   773.55081228],\n",
       "       [ 6391.46326301,  6391.46329848,  6391.46365283,  6391.4671691 ,\n",
       "         6391.50141307,  6391.91205329,  7925.97420252,  8018.97254987]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_initial = np.array([0]*dim)\n",
    "max_iters = 1000\n",
    "gammas = [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "lambdas = [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "losses = np.zeros([len(gammas), len(lambdas)])\n",
    "ws = np.zeros([len(gammas), len(lambdas), small_tx_std.shape[1]])\n",
    "\n",
    "small_labels_logistic =  np.copy(small_labels)\n",
    "small_labels_logistic[small_labels_logistic == -1] = 0\n",
    "\n",
    "for id0, gamma in enumerate(gammas):\n",
    "    for id1, lambda_ in enumerate(lambdas):\n",
    "        w, loss = reg_logistic_regression(small_labels_logistic, small_tx_std, lambda_, w_initial, max_iters, gamma) \n",
    "        losses[id0, id1] = loss\n",
    "        ws[id0, id1, :] = w\n",
    "\n",
    "# TODO : Formally get best params. In this case 1e-9 is the best so we'll see later\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method -- Least Squares GD\n",
      "params -- gamma = 0.0001, lambda = 0.0001, max_iters = 1000\n",
      "\n",
      "On a simple subset we obtain 40.60 percent accuracy\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_classification_error(subset_test_labels, small_test_x_std, ws[5][5])\n",
    "print(\"method -- Least Squares GD\")\n",
    "print(\"params -- gamma = {}, lambda = {}, max_iters = {}\".format(gammas[5], lambdas[5], max_iters))\n",
    "print()\n",
    "print(\"On a simple subset we obtain %.2f percent accuracy\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO Check impl, there's a problem here\n",
    "def cross_validation(y, tx, k_indices, k, lambda_):\n",
    "    \"\"\"return the loss of logistic regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    k_indices = k_indices.astype(int)\n",
    "    k_indices_train = np.array([])\n",
    "    \n",
    "    for i in range(k_indices.shape[0]):\n",
    "        if i != k:\n",
    "            k_indices_train = np.append(k_indices_train, k_indices[i])\n",
    "    \n",
    "    k_indices_test = k_indices[k]\n",
    "    \n",
    "    tx_train = tx[k_indices_train.astype(int)]\n",
    "    y_train = y[k_indices_train.astype(int)]\n",
    "    \n",
    "    tx_test = tx[k_indices_test.astype(int)]\n",
    "    y_test = y[k_indices_test.astype(int)]\n",
    "\n",
    "    # logistic regression\n",
    "    w = logistic_regression_gradient_descent(y_train, tx_train, lambda_)\n",
    "    \n",
    "    # calculate the loss for train and test data\n",
    "    loss_tr = compute_classification_error(y_train,tx_train,w)\n",
    "    loss_te = compute_classification_error(y_test,tx_test,w)\n",
    "\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plots import cross_validation_visualization\n",
    "# TODO Check impl, there's a problem here\n",
    "\n",
    "def cross_validation_demo(y, x):\n",
    "    verbose = True\n",
    "    seed = 1\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-4, 0, 30)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    print(\"Current run: {N} samples, {f} features\".format(N=x.shape[0], f=x.shape[1]))\n",
    "\n",
    "\n",
    "    # define lists to store the loss of training data and test data\n",
    "    final_losses_tr = []\n",
    "    final_losses_te = []\n",
    "\n",
    "    # cross validation: TODO\n",
    "    for idx, lambda_ in enumerate(lambdas):\n",
    "        losses_tr = []\n",
    "        losses_te = []\n",
    "        for k in range(k_fold):\n",
    "            loss_tr, loss_te = cross_validation(y, x, k_indices, k, lambda_)\n",
    "            losses_tr.append(loss_tr)\n",
    "            losses_te.append(loss_te)\n",
    "        final_losses_tr.append(np.mean(losses_tr))\n",
    "        final_losses_te.append(np.mean(losses_te))\n",
    "        if verbose:\n",
    "            print(\"Current lambda: {i} out of {j}\".format(i=idx, j=len(lambdas)))\n",
    "    \n",
    "    cross_validation_visualization(lambdas, final_losses_tr, final_losses_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current run: 1000 samples, 20 features\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logistic_regression_gradient_descent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-5d9b1c773aa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmall_tx_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmall_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcross_validation_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-6d2a7b5f0af7>\u001b[0m in \u001b[0;36mcross_validation_demo\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mlosses_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mloss_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mlosses_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mlosses_te\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-5592d1930a81>\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(y, tx, k_indices, k, lambda_)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# logistic regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# calculate the loss for train and test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logistic_regression_gradient_descent' is not defined"
     ]
    }
   ],
   "source": [
    "tx = small_tx_train\n",
    "y = small_labels\n",
    "cross_validation_demo(y,tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "* **Test** the model with the weights computed from the different learning algorithms to find the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validate\n",
    "\n",
    "* the hyperparameters for each algorithm\n",
    "    * `least_squares` \n",
    "    * `least_squares_GD` -> `gamma`\n",
    "    * `least_squares_SGD` -> `gamma`, `batch_size`\n",
    "    * `ridge_regression` -> `lambda_`\n",
    "    * `logistic_regression` -> `gamma`\n",
    "    * `reg_logistic_regression` -> `lambda_`, `gamma`\n",
    "* Also, if we use other features (e.g. polynomial), we should CV those as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "* Plot the train / test accuracies for the best set of algorithm + parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
