{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test suite for Project 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "from auxiliary import *\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "* Training set : `../data/train.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = '../data/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "easy_data_set = False # changer pour sélectionner un set de dim (1000,2) ou le vrai set\n",
    "\n",
    "if easy_data_set:\n",
    "    ids, y, X = load_easy_data(sub_sample=False)\n",
    "else:\n",
    "    ids, y, X = load_csv_data(train_path)\n",
    "    # Only 100% clean data\n",
    "    ranges = [(1,4), (7,12), (13,23), (29,30)]\n",
    "    keep_idx = build_idx(ranges)\n",
    "    X = X[:,keep_idx]\n",
    "\n",
    "x, mean_x, std_x = standardize(X)\n",
    "tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "tx = build_poly(x, 1)\n",
    "\n",
    "N = x.shape[0]\n",
    "D = x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check that we eliminated all unecessary features\n",
    "clean_data = [x for x in X if not -999. in x]\n",
    "len(clean_data)/len(X)*100 # percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Testing set : `../data/test.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_path  = '../data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# takes time, run only when needed\n",
    "ids_te, y_te, X_te = load_csv_data(test_path)\n",
    "\n",
    "ranges = [(1,4), (7,12), (13,23), (29,30)]\n",
    "keep_idx = build_idx(ranges)\n",
    "X_te = X_te[:,keep_idx]\n",
    "\n",
    "x_te, mean_x_te, std_x_te = standardize(X_te)\n",
    "tx_te = build_poly(x_te, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Long, run only if necessary\n",
    "# train_data = load_csv_data(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Long, run only if necessary\n",
    "# test_data = load_csv_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the data as pickles for more efficient reloading\n",
    "# Only run once to generate the pickle !\n",
    "# pickle.dump(test_data, open( 'test.p', 'wb' ))\n",
    "# pickle.dump(train_data, open( 'train.p', 'wb' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading the pickle back \n",
    "#train_data = pickle.load(open( 'train.p', 'rb' ))\n",
    "#test_data = pickle.load(open( 'test.p', 'rb' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory\n",
    "\n",
    "* Describe / Discover the data\n",
    "* Describe / Characterise the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 250000 samples.\n"
     ]
    }
   ],
   "source": [
    "## Data dimensions\n",
    "print(\"There are {} samples.\".format(len(train_data[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is defined as a set $D = \\{ (y_i, \\mathbf{x}_i, w_i) \\}$ with :\n",
    "* $y_i \\in \\{+1,-1\\}$ is the label (signal = `+1` or noise = `-1`)\n",
    "* $\\mathbf{x}_i \\in \\!R^d$ is a $d$-dimensional feature vector\n",
    "* $w_i \\in \\!R^+$ is a non-negative weight\n",
    "\n",
    "Note that $\\sum_{i\\in\\mathcal{S}} w_i = N_s$ and $\\sum_{i\\in\\mathcal{B}} w_i = N_b$ which are the *expected total number of signal and background events (resp.)*. This gives an estimate of how many events we should expect to classify for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The feature vector shape is \n",
    "train_data[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which means we have d features :\n",
    "d = train_data[2].shape[1]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids = train_data[0]\n",
    "labels  = train_data[1]\n",
    "x_train = train_data[2]\n",
    "\n",
    "if(labels.ndim<2):\n",
    "    labels = np.expand_dims(labels, axis=1) # expand the labels as array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ids = train_data[0]\n",
    "test_labels  = train_data[1]\n",
    "test_x = train_data[2]\n",
    "\n",
    "if(test_labels.ndim<2):\n",
    "    test_labels = np.expand_dims(test_labels, axis=1) # expand the labels as array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information about the features :\n",
    "\n",
    "* Variables are floating point unless specified otherwise.\n",
    "* All azimuthal φ angles are in radian in the [−π, +π[ range.\n",
    "* Energy, mass, momentum are all in GeV\n",
    "* All other variables are unit less\n",
    "* **Undefined values are `-999.0`**\n",
    "\n",
    "There are `primitive` (prefixed with `PRI`) values, directly measured from the collision, and `derived` (prefixed with `DER`) values which were computed from the primitive values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 17\n"
     ]
    }
   ],
   "source": [
    "PRI_features = x_train[:,:13]\n",
    "DER_features = x_train[:,13:]\n",
    "print(len(PRI_features[0]), len(DER_features[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "> This part is exploratory and informed the processing step done when loading the data. \n",
    "\n",
    "> !!!!!!!!!!!!!!!!!!!!!!!!!!! DO NOT RUN AGAIN !!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "* Split test / train\n",
    "* Clean useless features\n",
    "* [Scale / process data](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/) (e.g. scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much data is perfectly clean ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.245599999999996"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data = [x for x in x_train if not -999. in x]\n",
    "len(clean_data)/len(x_train)*100 # percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much data is clean in the primitive or derived classes ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.245599999999996"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRI_clean_data = [x for x in PRI_features if not -999. in x]\n",
    "len(PRI_clean_data)/len(PRI_features)*100 # percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.0172"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DER_clean_data = [x for x in DER_features if not -999. in x]\n",
    "len(DER_clean_data)/len(DER_features)*100 # percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage is almost the same ! This means there was not a lot of treatment error, using only `PRI` or only `DER` data won't make the data cleaner (but it might help reduce the complexity of the model ;) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there classes that contain a high probability of dirty data ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "per_feature_stats = []\n",
    "for i in range(d):\n",
    "    sz = len(x_train[:,i])\n",
    "    clean_ft = [x for x in x_train[:,i] if (x != -999.)] # c pa bo ... :'(\n",
    "    per_feature_stats.append(len(clean_ft) / sz * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  7.,   0.,   0.,   0.,   3.,   0.,   0.,   1.,   0.,  19.]),\n",
       " array([  29.0172 ,   36.11548,   43.21376,   50.31204,   57.41032,\n",
       "          64.5086 ,   71.60688,   78.70516,   85.80344,   92.90172,  100.     ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfklqjMEvSY0x+CWpMQa/JDXG4JekxvwvM2hP1ILGNHIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f1f87b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(per_feature_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a pretty good indication of the features that should be pruned from the data ! Notably :\n",
    "* `x_train[:, 4: 7]` (which are resp. `DER_deltaeta_jet_jet`, `DER_mass_jet_jet`, `DER_prodeta_jet_jet`)\n",
    "* `x_train[:,12]` (which is `DER_lep_eta_centrality`)\n",
    "* `x_train[:,26:29]` (which is the `PRI_jet_subleading_{pt,eta,phi}`)\n",
    "\n",
    "We can also consider removing `x_train[:,23:26]` (`PRI_jet_leading_{pt,eta,phi}`) which only has ~60% clean data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only 100% clean data\n",
    "ranges = [(1,4), (7,12), (13,23), (29,30)]\n",
    "keep_idx = build_idx(ranges)\n",
    "x_train_full_clean = x_train[:,keep_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With feature 1 and 23-26\n",
    "ranges = [(0,4), (7,12), (13,26), (29,30)]\n",
    "keep_idx = build_idx(ranges)\n",
    "x_train_partial_clean = x_train[:,keep_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With features 1-3\n",
    "ranges = [(1,4)]\n",
    "keep_idx = build_idx(ranges)\n",
    "x_train_small_features = x_train[:,keep_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the size of the sets, and expand the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(250000, 19)\n",
      "(250000, 23)\n",
      "(250000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train_full_clean.shape)\n",
    "print(x_train_partial_clean.shape)\n",
    "print(x_train_small_features.shape)\n",
    "\n",
    "tx_train = np.c_[np.ones((labels.shape[0], 1)), x_train]\n",
    "tx_train_full_clean = np.c_[np.ones((labels.shape[0], 1)), x_train_full_clean]\n",
    "tx_train_partial_clean = np.c_[np.ones((labels.shape[0], 1)), x_train_partial_clean]\n",
    "tx_train_small_features = np.c_[np.ones((labels.shape[0], 1)), x_train_small_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a very small set for testing the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Needs to be randomly sampled !\n",
    "sample_small_set = 1000\n",
    "\n",
    "small_tx_train = tx_train_full_clean[:sample_small_set, :]\n",
    "small_labels = labels[:sample_small_set].flatten()\n",
    "\n",
    "dim = small_tx_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "small_tx_std, mean_tx, std_tx = standardize(small_tx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38.333962050000004, 73.500211504812071, -5.6843418860808016e-17, 1.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_tx, std_tx, np.mean(small_tx_std), np.std(small_tx_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset_x_test_full_clean = test_x[sample_small_set:2*sample_small_set,:]\n",
    "subset_test_labels = test_labels[sample_small_set:2*sample_small_set].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "small_test_x_std, mean_test_x, std_test_x = standardize(small_tx_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Okay you can run again :) \n",
    "\n",
    "## Training\n",
    "\n",
    "* **Train** the model with the different learning algorithms\n",
    "    * `least_squares`\n",
    "    * `least_squares_GD`\n",
    "    * `least_squares_SGD`\n",
    "    * `ridge_regression`\n",
    "    * `logistic_regression`\n",
    "    * `reg_logistic_regression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_per_algorithm = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_LS, loss = least_squares(y,tx)\n",
    "_, err_tr = compute_classification_error(y, tx, w_LS)\n",
    "_, err_te = compute_classification_error(y_te, tx_te, w_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method   -- Least Squares\n",
      "\n",
      "\t -- Accuracy train = 39.01 percent \n",
      "\t -- Accuracy test  = 87.65 percent \n"
     ]
    }
   ],
   "source": [
    "accuracy_tr = 1 - err_tr\n",
    "accuracy_te = 1 - err_te\n",
    "print(\"method   -- Least Squares\")\n",
    "print()\n",
    "print(\"\\t -- Accuracy train = %.2f percent \" % (accuracy_tr*100))\n",
    "print(\"\\t -- Accuracy test  = %.2f percent \" % (accuracy_te*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_per_algorithm['LS'] = { \n",
    "    'accuracy' : {\n",
    "        'train' : accuracy_tr,\n",
    "        'test'  : accuracy_te,\n",
    "        'loss'  : loss,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least Squares with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_initial = np.array([0]*(D+1))\n",
    "max_iters = 100\n",
    "gammas = [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.2, 0.5]\n",
    "losses = []\n",
    "ws = [] \n",
    "\n",
    "for gamma in gammas:\n",
    "    w, loss = least_squares_GD(y, tx, w_initial, max_iters, gamma)\n",
    "    losses.append(loss)\n",
    "    ws.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_LS_GD = ws[np.argmin(losses)]\n",
    "_, err_tr = compute_classification_error(y, tx, w_LS_GD)\n",
    "_, err_te = compute_classification_error(y_te, tx_te, w_LS_GD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method   -- Least Squares GD\n",
      "params   -- gamma = 1e-09, max_iters = 100\n",
      "\n",
      "\t -- Accuracy train = 38.54 percent \n",
      "\t -- Accuracy test  = 95.20 percent \n"
     ]
    }
   ],
   "source": [
    "accuracy_tr = 1 - err_tr\n",
    "accuracy_te = 1 - err_te\n",
    "print(\"method   -- Least Squares GD\")\n",
    "print(\"params   -- gamma = {}, max_iters = {}\".format(gammas[0], max_iters))\n",
    "print()\n",
    "print(\"\\t -- Accuracy train = %.2f percent \" % (accuracy_tr*100))\n",
    "print(\"\\t -- Accuracy test  = %.2f percent \" % (accuracy_te*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_per_algorithm['LS_GD'] = { \n",
    "    'accuracy' : {\n",
    "        'train' : accuracy_tr,\n",
    "        'test'  : accuracy_te,\n",
    "        'loss'  : loss,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### least_squares_SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_initial = np.array([0]*(D+1))\n",
    "max_iters = 1000\n",
    "gammas = [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "batch_size = 1\n",
    "losses = []\n",
    "ws = []\n",
    "\n",
    "for gamma in gammas:\n",
    "    sgd_ws, sgd_losses = least_squares_SGD(y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "    losses.append(sgd_losses)\n",
    "    ws.append(sgd_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_LS_SGD = ws[np.argmin(losses)]\n",
    "_, err_tr = compute_classification_error(y, tx, w_LS_SGD)\n",
    "_, err_te = compute_classification_error(y_te, tx_te, w_LS_SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method   -- Least Squares SGD\n",
      "params   -- gamma = 1e-09, max_iters = 1000\n",
      "\n",
      "\t -- Accuracy train = 40.89 percent \n",
      "\t -- Accuracy test  = 91.36 percent \n"
     ]
    }
   ],
   "source": [
    "accuracy_tr = 1 - err_tr\n",
    "accuracy_te = 1 - err_te\n",
    "print(\"method   -- Least Squares SGD\")\n",
    "print(\"params   -- gamma = {}, max_iters = {}\".format(gammas[0], max_iters))\n",
    "print()\n",
    "print(\"\\t -- Accuracy train = %.2f percent \" % (accuracy_tr*100))\n",
    "print(\"\\t -- Accuracy test  = %.2f percent \" % (accuracy_te*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_per_algorithm['LS_SGD'] = { \n",
    "    'accuracy' : {\n",
    "        'train' : accuracy_tr,\n",
    "        'test'  : accuracy_te,\n",
    "        'loss'  : loss,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Least-Squares Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "degrees = [1, 3, 7, 12]\n",
    "losses = []\n",
    "ws = []\n",
    "        \n",
    "for ind, degree in enumerate(degrees):\n",
    "    phi = build_poly(x, degree)  \n",
    "        \n",
    "    w_degree_LS, loss_degree_LS = least_squares(y, phi)\n",
    "        \n",
    "    losses.append(loss_degree_LS)\n",
    "    ws.append(w_degree_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deg_POLY_LS = degrees[np.argmin(losses)]\n",
    "poly_tr = build_poly(x, deg_POLY_LS)\n",
    "poly_te = build_poly(x_te, deg_POLY_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_POLY_LS = ws[np.argmin(losses)]\n",
    "_, err_tr = compute_classification_error(y, poly_tr, w_POLY_LS)\n",
    "_, err_te = compute_classification_error(y_te, poly_te, w_POLY_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method   -- Polynomial Least-Squares\n",
      "params   -- degree = 12\n",
      "\n",
      "\t -- Accuracy train = 45.62 percent \n",
      "\t -- Accuracy test  = 79.82 percent \n"
     ]
    }
   ],
   "source": [
    "accuracy_tr = 1 - err_tr\n",
    "accuracy_te = 1 - err_te\n",
    "print(\"method   -- Polynomial Least-Squares\")\n",
    "print(\"params   -- degree = {}\".format(deg_POLY_LS))\n",
    "print()\n",
    "print(\"\\t -- Accuracy train = %.2f percent \" % (accuracy_tr*100))\n",
    "print(\"\\t -- Accuracy test  = %.2f percent \" % (accuracy_te*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_per_algorithm['LS_Poly'] = { \n",
    "    'accuracy' : {\n",
    "        'train' : accuracy_tr,\n",
    "        'test'  : accuracy_te,\n",
    "        'loss'  : loss,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ridge_regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion=0.2, degree=3, lambda=0.000, Training RMSE=0.082, Testing RMSE=13.491\n",
      "proportion=0.2, degree=3, lambda=0.000, Training RMSE=0.082, Testing RMSE=13.491\n",
      "proportion=0.2, degree=3, lambda=0.000, Training RMSE=0.082, Testing RMSE=13.491\n",
      "proportion=0.2, degree=3, lambda=0.000, Training RMSE=0.082, Testing RMSE=13.490\n",
      "proportion=0.2, degree=3, lambda=0.000, Training RMSE=0.082, Testing RMSE=13.488\n",
      "proportion=0.2, degree=3, lambda=0.000, Training RMSE=0.082, Testing RMSE=13.482\n",
      "proportion=0.2, degree=3, lambda=0.000, Training RMSE=0.082, Testing RMSE=13.473\n",
      "proportion=0.2, degree=3, lambda=0.000, Training RMSE=0.082, Testing RMSE=13.468\n",
      "proportion=0.2, degree=3, lambda=0.000, Training RMSE=0.082, Testing RMSE=13.467\n",
      "proportion=0.2, degree=3, lambda=0.000, Training RMSE=0.082, Testing RMSE=13.467\n",
      "proportion=0.2, degree=3, lambda=0.001, Training RMSE=0.082, Testing RMSE=13.467\n",
      "proportion=0.2, degree=3, lambda=0.007, Training RMSE=0.082, Testing RMSE=13.467\n",
      "proportion=0.2, degree=3, lambda=0.037, Training RMSE=0.082, Testing RMSE=13.467\n",
      "proportion=0.2, degree=3, lambda=0.193, Training RMSE=0.082, Testing RMSE=13.468\n",
      "proportion=0.2, degree=3, lambda=1.000, Training RMSE=0.082, Testing RMSE=13.471\n"
     ]
    }
   ],
   "source": [
    "lambdas = np.logspace(-10, 0, 15)\n",
    "\n",
    "ratio = 0.2\n",
    "degree = 3\n",
    "\n",
    "# split the data, and return train and test data\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "    \n",
    "# form train and test data with polynomial basis function\n",
    "phi_train = build_poly(x_train, degree)\n",
    "phi_test = build_poly(x_test, degree)\n",
    "    \n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "\n",
    "ws = []\n",
    "losses = []\n",
    "\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    w, loss = ridge_regression(y_train, phi_train, lambda_)\n",
    "    \n",
    "    ws.append(w)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    rmse_tr.append(compute_loss(y_train,phi_train,w))\n",
    "    rmse_te.append(compute_loss(y_test,phi_test,w))\n",
    "\n",
    "    print(\"proportion={p}, degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "            p=ratio, d=degree, l=lambda_ ,tr=rmse_tr[ind], te=rmse_te[ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deg_POLY_RR = degree\n",
    "poly_tr = build_poly(x, deg_POLY_RR)\n",
    "poly_te = build_poly(x_te, deg_POLY_RR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_RR = ws[np.argmin(losses)]\n",
    "_, err_tr = compute_classification_error(y, poly_tr, w_RR)\n",
    "_, err_te = compute_classification_error(y_te, poly_te, w_RR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method   -- Ridge Regression\n",
      "params   -- lambda = 5.179474679231202e-10, degree = 3\n",
      "\n",
      "\t -- Accuracy train = 42.00 percent \n",
      "\t -- Accuracy test  = 75.97 percent \n"
     ]
    }
   ],
   "source": [
    "accuracy_tr = 1 - err_tr\n",
    "accuracy_te = 1 - err_te\n",
    "print(\"method   -- Ridge Regression\")\n",
    "print(\"params   -- lambda = {}, degree = {}\".format(lambdas[np.argmin(losses)], deg_POLY_RR))\n",
    "print()\n",
    "print(\"\\t -- Accuracy train = %.2f percent \" % (accuracy_tr*100))\n",
    "print(\"\\t -- Accuracy test  = %.2f percent \" % (accuracy_te*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_per_algorithm['RR'] = { \n",
    "    'accuracy' : {\n",
    "        'train' : accuracy_tr,\n",
    "        'test'  : accuracy_te,\n",
    "        'loss'  : loss,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### logistic_regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Dylan/Documents/Cours/MT_MA3/Machine Learning/ML_Projects/project1/src/auxiliary.py:71: RuntimeWarning: overflow encountered in exp\n",
      "  loss = np.sum(np.log(1+np.exp(tx.dot(w))) - y*(tx.dot(w)))\n",
      "/Users/Dylan/Documents/Cours/MT_MA3/Machine Learning/ML_Projects/project1/src/auxiliary.py:68: RuntimeWarning: overflow encountered in exp\n",
      "  return 1. / (1. + np.exp(-t))\n",
      "/Users/Dylan/Documents/Cours/MT_MA3/Machine Learning/ML_Projects/project1/src/auxiliary.py:199: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if abs(a-b) < threshold:\n"
     ]
    }
   ],
   "source": [
    "w_initial = np.array([0]*(D+1))\n",
    "max_iters = 1000\n",
    "gammas = [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.2, 0.5]\n",
    "losses = []\n",
    "ws = []\n",
    "\n",
    "for gamma in gammas:\n",
    "    w, loss = logistic_regression(y, tx, w_initial, max_iters, gamma)\n",
    "    losses.append(loss)\n",
    "    ws.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_LR = ws[np.argmin(losses)]\n",
    "_, err_tr = compute_classification_error(y, tx, w_LR)\n",
    "_, err_te = compute_classification_error(y_te, tx_te, w_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method   -- Logistic Regression\n",
      "params   -- gamma = 1e-06\n",
      "\n",
      "\t -- Accuracy train = 73.87 percent \n",
      "\t -- Accuracy test  = 27.67 percent \n"
     ]
    }
   ],
   "source": [
    "accuracy_tr = 1 - err_tr\n",
    "accuracy_te = 1 - err_te\n",
    "print(\"method   -- Logistic Regression\")\n",
    "print(\"params   -- gamma = {}\".format(gammas[np.argmin(losses)]))\n",
    "print()\n",
    "print(\"\\t -- Accuracy train = %.2f percent \" % (accuracy_tr*100))\n",
    "print(\"\\t -- Accuracy test  = %.2f percent \" % (accuracy_te*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_per_algorithm['LR'] = { \n",
    "    'accuracy' : {\n",
    "        'train' : accuracy_tr,\n",
    "        'test'  : accuracy_te,\n",
    "        'loss'  : loss,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reg_logistic_regression\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-4c04ef616ae9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw_initial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmax_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgammas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1e-9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlambdas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1e-9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dim' is not defined"
     ]
    }
   ],
   "source": [
    "w_initial = np.array([0]*dim)\n",
    "max_iters = 1000\n",
    "gammas = [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "lambdas = [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "losses = np.zeros([len(gammas), len(lambdas)])\n",
    "ws = np.zeros([len(gammas), len(lambdas), small_tx_std.shape[1]])\n",
    "\n",
    "small_labels_logistic =  np.copy(small_labels)\n",
    "small_labels_logistic[small_labels_logistic == -1] = 0\n",
    "\n",
    "for id0, gamma in enumerate(gammas):\n",
    "    for id1, lambda_ in enumerate(lambdas):\n",
    "        w, loss = reg_logistic_regression(small_labels_logistic, small_tx_std, lambda_, w_initial, max_iters, gamma) \n",
    "        losses[id0, id1] = loss\n",
    "        ws[id0, id1, :] = w\n",
    "\n",
    "# TODO : Formally get best params. In this case 1e-9 is the best so we'll see later\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method -- Least Squares GD\n",
      "params -- gamma = 0.0001, lambda = 0.0001, max_iters = 1000\n",
      "\n",
      "On a simple subset we obtain 40.60 percent accuracy\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_classification_error(subset_test_labels, small_test_x_std, ws[5][5])\n",
    "print(\"method -- Least Squares GD\")\n",
    "print(\"params -- gamma = {}, lambda = {}, max_iters = {}\".format(gammas[5], lambdas[5], max_iters))\n",
    "print()\n",
    "print(\"On a simple subset we obtain %.2f percent accuracy\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### CV on Least Squares\n",
    "\n",
    "> Pour la beauté du geste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_split_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"polynomial regression with different split ratios and different degrees.\"\"\"\n",
    "    # split the data, and return train and test data: TODO\n",
    "    x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "\n",
    "    # form train and test data with polynomial basis function: TODO\n",
    "    phi_train = build_poly(x_train, degree)\n",
    "    phi_test = build_poly(x_test, degree)\n",
    "    \n",
    "    # calcualte weight through least square.: TODO\n",
    "    w,loss = least_squares(y_train, phi_train)\n",
    "\n",
    "    # calculate RMSE for train and test data,\n",
    "    # and store them in rmse_tr and rmse_te respectively: TODO\n",
    "    loss_tr = compute_loss(y_train,phi_train,w)\n",
    "    loss_te = compute_loss(y_test,phi_test,w)\n",
    "    \n",
    "    print(\"proportion={p}, degree={d}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "          p=ratio, d=degree, tr=loss_tr, te=loss_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 6\n",
    "degrees = [1, 3, 7, 12]\n",
    "split_ratios = [0.9, 0.5, 0.1]\n",
    "\n",
    "for split_ratio in split_ratios:\n",
    "    for degree in degrees:\n",
    "        train_test_split_demo(x, y, degree, split_ratio, seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation with logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_split(x, y, degree, ratio, gamma, seed):\n",
    "    \"\"\"polynomial regression with different split ratios and different degrees.\"\"\"\n",
    "    # split the data, and return train and test data: TODO\n",
    "    x_train, x_test, y_train, y_test = split_data(x, y, ratio, seed)\n",
    "\n",
    "    # form train and test data with polynomial basis function: TODO\n",
    "    phi_train = build_poly(x_train, degree)\n",
    "    phi_test = build_poly(x_test, degree)\n",
    "    \n",
    "    w_initial = np.array([0]*(phi_train.shape[1]))\n",
    "    max_iters = 1000\n",
    "\n",
    "    \n",
    "    # calcualte weight through least square.: TODO\n",
    "    w,loss_tr = logistic_regression(y_train, phi_train, w_initial, max_iters, gamma)\n",
    "    \n",
    "    loss_te = compute_logistic_loss(y_test, phi_test, w)\n",
    "    # compute error and loss for train and test data\n",
    "    error, ratio_error_train = compute_classification_error(y_train, phi_train, w)\n",
    "    error, ratio_error_test = compute_classification_error(y_test, phi_test, w)\n",
    "\n",
    "    \n",
    "    print(\"proportion={p}, degree={d}, gamma={g}, Train loss, err_ratio=({tr:.3f},{er_tr:.3f}), Test loss, err_ratio=({te:.3f},{er_te:.3f})\".format(\n",
    "          p=ratio, d=degree, g=gamma, tr=loss_tr, te=loss_te, er_tr=ratio_error_train, er_te=ratio_error_test))\n",
    "    return w, ratio_error_train, ratio_error_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 6\n",
    "degrees = [1, 3, 7, 12]\n",
    "split_ratios = [0.9, 0.8, 0.5, 0.1]\n",
    "gammas = [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.2, 0.5]\n",
    "\n",
    "ws = []\n",
    "ratio_err_trains = []\n",
    "ratio_err_tests = []\n",
    "\n",
    "for split_ratio in split_ratios:\n",
    "    for degree in degrees:\n",
    "        for gamma in gammas:\n",
    "            w, ratio_error_train, ratio_error_test = logistic_regression_split(x, y, degree, split_ratio, gamma, seed)\n",
    "            ws.append(w)\n",
    "            ratio_err_trains.append(ratio_error_train)\n",
    "            ratio_err_tests.append(ratio_error_test)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation with regularized logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reg_logistic_regression_split(x, y, degree, ratio, gamma, lambda_, seed):\n",
    "    \"\"\"polynomial regression with different split ratios and different degrees.\"\"\"\n",
    "    # split the data, and return train and test data: TODO\n",
    "    x_train, x_test, y_train, y_test = split_data(x, y, ratio, seed)\n",
    "\n",
    "    # form train and test data with polynomial basis function: TODO\n",
    "    phi_train = build_poly(x_train, degree)\n",
    "    phi_test = build_poly(x_test, degree)\n",
    "    \n",
    "    w_initial = np.array([0]*(phi_train.shape[1]))\n",
    "    max_iters = 1000\n",
    "\n",
    "    \n",
    "    # calcualte weight through logistic regression\n",
    "    w, loss_tr = reg_logistic_regression(y_train, phi_train, lambda_, w_initial, max_iters, gamma, SGD=False)\n",
    "    \n",
    "    loss_te = compute_logistic_loss(y_test, phi_test, w)\n",
    "    # compute error and loss for train and test data\n",
    "    error, ratio_error_train = compute_classification_error(y_train, phi_train, w)\n",
    "    error, ratio_error_test = compute_classification_error(y_test, phi_test, w)\n",
    "\n",
    "    \n",
    "    print(\"prop={p}, deg={d}, g={g:.3f}, l={l:.3f}, Train loss, err_ratio=({tr:.3f},{er_tr:.3f}), Test loss, err_ratio=({te:.3f},{er_te:.3f})\".format(\n",
    "          p=ratio, d=degree, g=gamma, l=lambda_, tr=loss_tr, te=loss_te, er_tr=ratio_error_train, er_te=ratio_error_test))\n",
    "    return w, ratio_error_train, ratio_error_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 6\n",
    "degrees = [1, 7]\n",
    "split_ratios = [0.9]\n",
    "gammas = [1e-9, 1e-5, 1e-1, 0.5]\n",
    "lambdas = np.logspace(-5, 0, 2)\n",
    "\n",
    "ws = []\n",
    "ratio_err_trains = []\n",
    "ratio_err_tests = []\n",
    "\n",
    "for split_ratio in split_ratios:\n",
    "    for degree in degrees:\n",
    "        for gamma in gammas:\n",
    "            for lambda_ in lambdas:\n",
    "                w, ratio_error_train, ratio_error_test = reg_logistic_regression_split(x, y, degree, split_ratio, gamma, lambda_, seed)\n",
    "                ws.append(w)\n",
    "                ratio_err_trains.append(ratio_error_train)\n",
    "                ratio_err_tests.append(ratio_error_test)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_reg_log_reg = ws[np.argmin(ratio_err_tests)]\n",
    "np.argmin(ratio_err_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "* Plot the train / test accuracies for the best set of algorithm + parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_te = predict_labels(w_reg_log_reg, tx_te)\n",
    "y_te[y_te==0] = -1\n",
    "create_csv_submission(ids_te, y_te, 'submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
